{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d5b50b0-2d16-45b9-94c8-412d3f5b9785",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertModel, AdamW\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56ca01d3-5a2e-44dc-bea8-adcf19cca430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading balls....\n",
      "7586  balls are loaded\n",
      "\n",
      "1576\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>instance_id</th>\n",
       "      <th>lemma</th>\n",
       "      <th>pos</th>\n",
       "      <th>word</th>\n",
       "      <th>sentence_text</th>\n",
       "      <th>sense_id</th>\n",
       "      <th>formatted_sense_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>d000.s008</td>\n",
       "      <td>d000.s008.t000</td>\n",
       "      <td>be</td>\n",
       "      <td>VERB</td>\n",
       "      <td>Are</td>\n",
       "      <td>Are there other , cheaper communications techn...</td>\n",
       "      <td>be%2:42:00::</td>\n",
       "      <td>be.v.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>d000.s067</td>\n",
       "      <td>d000.s067.t010</td>\n",
       "      <td>be</td>\n",
       "      <td>VERB</td>\n",
       "      <td>are</td>\n",
       "      <td>But even if that other plant employs the same ...</td>\n",
       "      <td>be%2:42:00::</td>\n",
       "      <td>be.v.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>d000.s070</td>\n",
       "      <td>d000.s070.t002</td>\n",
       "      <td>be</td>\n",
       "      <td>VERB</td>\n",
       "      <td>are</td>\n",
       "      <td>In what section of the country are you located ?</td>\n",
       "      <td>be%2:42:05::</td>\n",
       "      <td>be.v.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>d000.s071</td>\n",
       "      <td>d000.s071.t000</td>\n",
       "      <td>be</td>\n",
       "      <td>VERB</td>\n",
       "      <td>Are</td>\n",
       "      <td>Are you in a rural or urban area ?</td>\n",
       "      <td>be%2:42:05::</td>\n",
       "      <td>be.v.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>893</th>\n",
       "      <td>d000.s110</td>\n",
       "      <td>d000.s110.t000</td>\n",
       "      <td>be</td>\n",
       "      <td>VERB</td>\n",
       "      <td>are</td>\n",
       "      <td>There are two sides of a coin for this decision .</td>\n",
       "      <td>be%2:42:00::</td>\n",
       "      <td>be.v.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    sentence_id     instance_id lemma   pos word  \\\n",
       "52    d000.s008  d000.s008.t000    be  VERB  Are   \n",
       "554   d000.s067  d000.s067.t010    be  VERB  are   \n",
       "564   d000.s070  d000.s070.t002    be  VERB  are   \n",
       "566   d000.s071  d000.s071.t000    be  VERB  Are   \n",
       "893   d000.s110  d000.s110.t000    be  VERB  are   \n",
       "\n",
       "                                         sentence_text      sense_id  \\\n",
       "52   Are there other , cheaper communications techn...  be%2:42:00::   \n",
       "554  But even if that other plant employs the same ...  be%2:42:00::   \n",
       "564   In what section of the country are you located ?  be%2:42:05::   \n",
       "566                 Are you in a rural or urban area ?  be%2:42:05::   \n",
       "893  There are two sides of a coin for this decision .  be%2:42:00::   \n",
       "\n",
       "    formatted_sense_id  \n",
       "52             be.v.00  \n",
       "554            be.v.00  \n",
       "564            be.v.05  \n",
       "566            be.v.05  \n",
       "893            be.v.00  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# First importing data\n",
    "semcor_training_xml = 'WSD_Evaluation_Framework/Training_Corpora/Semcor/semcor.data.xml'\n",
    "\n",
    "tree = ET.parse(semcor_training_xml)\n",
    "root = tree.getroot()\n",
    "data = []\n",
    "for text in root.findall('text'):\n",
    "    for sentence in text.findall('sentence'):\n",
    "        sentence_id = sentence.get('id')\n",
    "        sentence_text = ' '.join([element.text for element in sentence])\n",
    "        for instance in sentence.findall('instance'):\n",
    "            instance_id = instance.get('id')\n",
    "            lemma = instance.get('lemma')\n",
    "            pos = instance.get('pos')\n",
    "            word = instance.text\n",
    "            data.append([sentence_id, instance_id, lemma, pos, word, sentence_text])\n",
    "\n",
    "# Create a DataFrame\n",
    "columns = ['sentence_id', 'instance_id', 'lemma', 'pos', 'word', 'sentence_text']\n",
    "xml_data = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "semcor_training_gold_key = 'WSD_Evaluation_Framework/Training_Corpora/Semcor/semcor.gold.key.txt'\n",
    "\n",
    "# Parse the gold key file\n",
    "gold_data = []\n",
    "with open(semcor_training_gold_key, 'r') as file:\n",
    "    for line in file:\n",
    "        parts = line.strip().split()\n",
    "        instance_id = parts[0]\n",
    "        sense_id = parts[1]\n",
    "        gold_data.append([instance_id, sense_id])\n",
    "\n",
    "# Create a DataFrame\n",
    "gold_columns = ['instance_id', 'sense_id']\n",
    "gold_df = pd.DataFrame(gold_data, columns=gold_columns)\n",
    "\n",
    "merged_data = pd.merge(xml_data, gold_df, on='instance_id', how='inner')\n",
    "\n",
    "def format_sense_id(sense_id):\n",
    "    try:\n",
    "        # Split the sense_id by '%'\n",
    "        parts = sense_id.split('%')\n",
    "        if len(parts) != 2:\n",
    "            return None  # Invalid format\n",
    "        \n",
    "        lemma = parts[0]\n",
    "        sense_info = parts[1].split(':')\n",
    "        \n",
    "        # Ensure there are enough parts\n",
    "        if len(sense_info) < 2:\n",
    "            return None  # Invalid format\n",
    "        \n",
    "        # Convert WordNet POS tags\n",
    "        pos_mapping = {'1': 'n', '2': 'v', '3': 'a', '4': 'r'}\n",
    "        pos = pos_mapping.get(sense_info[0], None)\n",
    "        if not pos:\n",
    "            return None  # Invalid POS\n",
    "        \n",
    "        # Combine to form a WordNet synset id\n",
    "        synset_id = f\"{lemma}.{pos}.{sense_info[2].zfill(2)}\"\n",
    "        return synset_id\n",
    "    except Exception as e:\n",
    "        # If any error occurs, return None\n",
    "        return None\n",
    "\n",
    "# def is_valid_sense(sense_id):\n",
    "#     formatted_sense_id = format_sense_id(sense_id)\n",
    "#     if not formatted_sense_id:\n",
    "#         return False\n",
    "#     try:\n",
    "#         wn.synset(formatted_sense_id)\n",
    "#         return True\n",
    "#     except nltk.corpus.reader.wordnet.WordNetError:\n",
    "#         return False\n",
    "\n",
    "# Apply the function to the merged dataset\n",
    "merged_data['formatted_sense_id'] = merged_data['sense_id'].apply(format_sense_id)\n",
    "# merged_data['valid_sense'] = merged_data['formatted_sense_id'].apply(is_valid_sense)\n",
    "\n",
    "def load_ball_embeddings(bFile=''):\n",
    "    \"\"\"\n",
    "    :param bFile:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    print(\"loading balls....\")\n",
    "    nball_list = []\n",
    "    with open(bFile, 'r') as w2v:\n",
    "        for line in w2v.readlines():\n",
    "            wlst = line.strip().split()\n",
    "            nball_list.append(wlst[0])\n",
    "    print(len(nball_list),' balls are loaded\\n')\n",
    "    return nball_list\n",
    "\n",
    "nball_list = load_ball_embeddings('training_set\\word2vec.txt')\n",
    "nball_list.append('be.v.05')\n",
    "nball_list.append('be.v.00')\n",
    "small_set = merged_data[merged_data['formatted_sense_id'].isin(nball_list)]\n",
    "print(len(small_set))\n",
    "display(small_set.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fc6ba84-537f-4438-bbdb-8dbd963e8094",
   "metadata": {},
   "outputs": [],
   "source": [
    "keys_to_keep = ['lemma', 'word', 'sentence_text', 'formatted_sense_id']\n",
    "small_set = small_set[keys_to_keep]\n",
    "\n",
    "def generate_nball_dict(nball_file=''):\n",
    "    nball_embeddings = {\n",
    "        \"be.v.05\": np.random.rand(128).astype(np.float32),\n",
    "        \"be.v.00\": np.random.rand(128).astype(np.float32)\n",
    "    }\n",
    "    return nball_embeddings\n",
    "\n",
    "nball_embeddings = generate_nball_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "061d3428-f889-4b84-9698-a32254dc5434",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemma</th>\n",
       "      <th>word</th>\n",
       "      <th>sentence_text</th>\n",
       "      <th>formatted_sense_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>be</td>\n",
       "      <td>Are</td>\n",
       "      <td>Are there other , cheaper communications techn...</td>\n",
       "      <td>be.v.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>554</th>\n",
       "      <td>be</td>\n",
       "      <td>are</td>\n",
       "      <td>But even if that other plant employs the same ...</td>\n",
       "      <td>be.v.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>564</th>\n",
       "      <td>be</td>\n",
       "      <td>are</td>\n",
       "      <td>In what section of the country are you located ?</td>\n",
       "      <td>be.v.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>566</th>\n",
       "      <td>be</td>\n",
       "      <td>Are</td>\n",
       "      <td>Are you in a rural or urban area ?</td>\n",
       "      <td>be.v.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>893</th>\n",
       "      <td>be</td>\n",
       "      <td>are</td>\n",
       "      <td>There are two sides of a coin for this decision .</td>\n",
       "      <td>be.v.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    lemma word                                      sentence_text  \\\n",
       "52     be  Are  Are there other , cheaper communications techn...   \n",
       "554    be  are  But even if that other plant employs the same ...   \n",
       "564    be  are   In what section of the country are you located ?   \n",
       "566    be  Are                 Are you in a rural or urban area ?   \n",
       "893    be  are  There are two sides of a coin for this decision .   \n",
       "\n",
       "    formatted_sense_id  \n",
       "52             be.v.00  \n",
       "554            be.v.00  \n",
       "564            be.v.05  \n",
       "566            be.v.05  \n",
       "893            be.v.00  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d77304d3-4c50-4ccb-b164-331409086688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(\"prajjwal1/bert-tiny\")\n",
    "\n",
    "# Tokenize sentences to get input_ids and attention masks\n",
    "def tokenize_and_find_index(row):\n",
    "    # Convert the sentence to string in case it's not (handling NaN or None)\n",
    "    sentence = str(row['sentence_text'])\n",
    "    word = str(row['word'])  # Ensure the target word is also a string\n",
    "    \n",
    "    # Tokenize the sentence\n",
    "    tokens = tokenizer(sentence, padding='max_length', max_length=512, truncation=True, return_tensors=\"pt\")\n",
    "    input_ids = tokens['input_ids'][0]\n",
    "    \n",
    "    # Tokenize the target word separately to find its first occurrence in the sentence\n",
    "    word_tokens = tokenizer.tokenize(word)\n",
    "    \n",
    "    # Find the start index of the first complete occurrence of the word tokens in the input_ids\n",
    "    for i in range(len(input_ids) - len(word_tokens) + 1):\n",
    "        if input_ids[i:i+len(word_tokens)].tolist() == tokenizer.convert_tokens_to_ids(word_tokens):\n",
    "            return tokens['input_ids'], tokens['attention_mask'], i\n",
    "    \n",
    "    # If word not found, handle it appropriately, here returning -1\n",
    "    return tokens['input_ids'], tokens['attention_mask'], -1\n",
    "\n",
    "\n",
    "small_set[['input_ids', 'attention_mask', 'word_index']] = small_set.apply(tokenize_and_find_index, axis=1, result_type='expand')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f2279313-8760-4b1c-b334-7c58e4834a40",
   "metadata": {},
   "outputs": [],
   "source": [
    "sense_labels = list(nball_embeddings.keys())\n",
    "sense_embeddings = torch.tensor(np.array([nball_embeddings[label] for label in sense_labels]), dtype=torch.float32)\n",
    "sense_index = {sense: idx for idx, sense in enumerate(sense_labels)}\n",
    "\n",
    "small_set['sense_idx'] = small_set['formatted_sense_id'].map(sense_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3fad72ac-1118-4d91-8212-000feb083a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# small_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87107920-e79b-418d-8039-d4aa2f1fdc31",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "# Assuming input_ids, attention_mask, and word_index have been properly processed\n",
    "all_input_ids = torch.cat(small_set['input_ids'].tolist())\n",
    "all_attention_masks = torch.cat(small_set['attention_mask'].tolist())\n",
    "all_word_indices = torch.tensor(small_set['word_index'].tolist())\n",
    "all_senses = torch.tensor(small_set['sense_idx'].tolist())\n",
    "\n",
    "# Create a TensorDataset\n",
    "dataset = TensorDataset(all_input_ids, all_attention_masks, all_word_indices, all_senses)\n",
    "\n",
    "# Use DataLoader to handle batching\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "09e223da-1253-4403-abb3-879e7617b048",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch 1, Loss: 1.008372647356866\n",
      "Epoch 2, Loss: 0.822199387447483\n",
      "Epoch 3, Loss: 0.7519104360475153\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Set the device to CUDA if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Load BERT model\n",
    "model = BertModel.from_pretrained(\"prajjwal1/bert-tiny\").to(device)\n",
    "model.train() \n",
    "sense_embeddings = sense_embeddings.to(device)  # Move sense embeddings to GPU\n",
    "\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for batch in dataloader:\n",
    "        # Send batch data to the device (GPU)\n",
    "        batch_input_ids, batch_attention_masks, batch_word_indices, batch_sense_indices = [b.to(device) for b in batch]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=batch_input_ids, attention_mask=batch_attention_masks)\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        \n",
    "        # Retrieve embeddings for specific word indices\n",
    "        word_embeddings = torch.stack([hidden_states[i, idx, :] for i, idx in enumerate(batch_word_indices)])\n",
    "        \n",
    "        # Retrieve the corresponding sense embeddings\n",
    "        target_embeddings = sense_embeddings[batch_sense_indices]\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = loss_fn(word_embeddings, target_embeddings)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, Loss: {total_loss / len(dataloader)}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8483b47-814c-42aa-b867-556e0edfd4b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "model.eval()\n",
    "prediction_dataloader = DataLoader(dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "predicted_sense_ids = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in prediction_dataloader:\n",
    "        batch_input_ids, batch_attention_masks, batch_word_indices, _ = [b.to(device) for b in batch]\n",
    "\n",
    "        outputs = model(input_ids=batch_input_ids, attention_mask=batch_attention_masks)\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        \n",
    "        word_embeddings = torch.stack([hidden_states[i, idx, :] for i, idx in enumerate(batch_word_indices)])\n",
    "        \n",
    "        # Calculate cosine similarities between word embeddings and sense embeddings\n",
    "        cosine_similarities = F.cosine_similarity(word_embeddings.unsqueeze(1), sense_embeddings.unsqueeze(0), dim=-1)\n",
    "        \n",
    "        # Get the index of the closest sense embedding\n",
    "        predicted_indices = torch.argmax(cosine_similarities, dim=1)\n",
    "        \n",
    "        predicted_sense_ids.extend(predicted_indices.cpu().numpy())\n",
    "\n",
    "small_set['predicted_sense_id'] = [sense_labels[idx] for idx in predicted_sense_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fccd6d05-1298-4018-8353-82e11b8b31d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 82.80%\n"
     ]
    }
   ],
   "source": [
    "correct_predictions = small_set['formatted_sense_id'] == small_set['predicted_sense_id']\n",
    "accuracy = correct_predictions.mean()\n",
    "\n",
    "print(f\"Model Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c7f72b6e-8e9d-40ca-91bc-bf821a8f1fe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_text</th>\n",
       "      <th>formatted_sense_id</th>\n",
       "      <th>predicted_sense_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4593</th>\n",
       "      <td>He did not , however , settle back into acquie...</td>\n",
       "      <td>be.v.00</td>\n",
       "      <td>be.v.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6735</th>\n",
       "      <td>We got to one house where there were five sece...</td>\n",
       "      <td>be.v.05</td>\n",
       "      <td>be.v.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7240</th>\n",
       "      <td>What had been the ambassador 's suite was now ...</td>\n",
       "      <td>be.v.00</td>\n",
       "      <td>be.v.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7457</th>\n",
       "      <td>We were there at a moment when the situation i...</td>\n",
       "      <td>be.v.05</td>\n",
       "      <td>be.v.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7773</th>\n",
       "      <td>Mr. Keo , once a diplomat in Paris and Washing...</td>\n",
       "      <td>be.v.00</td>\n",
       "      <td>be.v.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225398</th>\n",
       "      <td>As evening approached and Palmer finished his ...</td>\n",
       "      <td>be.v.00</td>\n",
       "      <td>be.v.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225403</th>\n",
       "      <td>But Palmer knew , as did everybody else at Aug...</td>\n",
       "      <td>be.v.00</td>\n",
       "      <td>be.v.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225425</th>\n",
       "      <td>On the final round at Pensacola , the luck of ...</td>\n",
       "      <td>be.v.00</td>\n",
       "      <td>be.v.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225441</th>\n",
       "      <td>It was a dismal , drizzly day but a good one o...</td>\n",
       "      <td>be.v.00</td>\n",
       "      <td>be.v.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225960</th>\n",
       "      <td>They felt that they were relaxing as much as t...</td>\n",
       "      <td>be.v.05</td>\n",
       "      <td>be.v.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>271 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            sentence_text formatted_sense_id  \\\n",
       "4593    He did not , however , settle back into acquie...            be.v.00   \n",
       "6735    We got to one house where there were five sece...            be.v.05   \n",
       "7240    What had been the ambassador 's suite was now ...            be.v.00   \n",
       "7457    We were there at a moment when the situation i...            be.v.05   \n",
       "7773    Mr. Keo , once a diplomat in Paris and Washing...            be.v.00   \n",
       "...                                                   ...                ...   \n",
       "225398  As evening approached and Palmer finished his ...            be.v.00   \n",
       "225403  But Palmer knew , as did everybody else at Aug...            be.v.00   \n",
       "225425  On the final round at Pensacola , the luck of ...            be.v.00   \n",
       "225441  It was a dismal , drizzly day but a good one o...            be.v.00   \n",
       "225960  They felt that they were relaxing as much as t...            be.v.05   \n",
       "\n",
       "       predicted_sense_id  \n",
       "4593              be.v.05  \n",
       "6735              be.v.00  \n",
       "7240              be.v.05  \n",
       "7457              be.v.00  \n",
       "7773              be.v.05  \n",
       "...                   ...  \n",
       "225398            be.v.05  \n",
       "225403            be.v.05  \n",
       "225425            be.v.05  \n",
       "225441            be.v.05  \n",
       "225960            be.v.00  \n",
       "\n",
       "[271 rows x 3 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add a new column to the DataFrame\n",
    "selected_columns = small_set[['sentence_text', 'formatted_sense_id', 'predicted_sense_id']]\n",
    "selected_data = selected_columns[selected_columns['formatted_sense_id'] != selected_columns['predicted_sense_id']]\n",
    "\n",
    "# Display the filtered DataFrame\n",
    "selected_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7180bb90-9d46-4b79-baf3-b40cdeffbcc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (annotated-transformer)",
   "language": "python",
   "name": "annotated-transformer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
