{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a09f8e0-ea1e-4dfa-a668-b7680eeab8f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BertTokenizer, BertModel, AdamW\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import xml.etree.ElementTree as ET\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn\n",
    "from collections import namedtuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42428dc0-b513-4a9a-a57c-5fc75b727783",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "31\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def generate_balls(num_layers, current_idx, current_center, current_radius, balls, dim):\n",
    "    if num_layers == 0:\n",
    "        return\n",
    "\n",
    "    # Define the distance between the centers of two sibling balls\n",
    "    separation = 2 * current_radius / 3\n",
    "\n",
    "    # Choose a random direction for the first child ball's center\n",
    "    direction = np.random.randn(dim)\n",
    "    direction = direction / np.linalg.norm(direction)  # Normalize the direction vector\n",
    "\n",
    "    # Calculate the centers for the two child balls\n",
    "    center1 = current_center + direction * separation\n",
    "    center2 = current_center - direction * separation\n",
    "\n",
    "    # Calculate the radius for the child balls\n",
    "    child_radius = current_radius / 3\n",
    "\n",
    "    # Assign the child balls to the dictionary with updated indices\n",
    "    left_idx = 2 * current_idx + 1\n",
    "    right_idx = 2 * current_idx + 2\n",
    "    balls[left_idx] = {'center': center1, 'radius': child_radius}\n",
    "    balls[right_idx] = {'center': center2, 'radius': child_radius}\n",
    "\n",
    "    # Recursive call for each child\n",
    "    generate_balls(num_layers - 1, left_idx, center1, child_radius, balls, dim)\n",
    "    generate_balls(num_layers - 1, right_idx, center2, child_radius, balls, dim)\n",
    "\n",
    "# Root ball parameters\n",
    "dim = 3\n",
    "layer = 4\n",
    "initial_radius = 10\n",
    "initial_center = np.zeros(dim)\n",
    "\n",
    "# Initialize the dictionary to hold all the balls\n",
    "balls = {0: {'center': initial_center, 'radius': initial_radius}}\n",
    "\n",
    "# Generate the balls recursively\n",
    "generate_balls(layer, 0, initial_center, initial_radius, balls, dim)\n",
    "\n",
    "# Check how many balls we have generated\n",
    "ball_num = len(balls)\n",
    "print(ball_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "47b34e67-8224-4e52-a941-b317f9362fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings for loading training data\n",
    "semcor_training_xml_path = 'WSD_Evaluation_Framework/Training_Corpora/Semcor/semcor.data.xml'\n",
    "semcor_training_gk_path = 'WSD_Evaluation_Framework/Training_Corpora/Semcor/semcor.gold.key.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "48369b32-f088-4dfa-926c-b512a8b214ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>instance_id</th>\n",
       "      <th>lemma</th>\n",
       "      <th>pos</th>\n",
       "      <th>word</th>\n",
       "      <th>sentence_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d000.s000</td>\n",
       "      <td>d000.s000.t000</td>\n",
       "      <td>long</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>long</td>\n",
       "      <td>How long has it been since you reviewed the ob...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d000.s000</td>\n",
       "      <td>d000.s000.t001</td>\n",
       "      <td>be</td>\n",
       "      <td>VERB</td>\n",
       "      <td>been</td>\n",
       "      <td>How long has it been since you reviewed the ob...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d000.s000</td>\n",
       "      <td>d000.s000.t002</td>\n",
       "      <td>review</td>\n",
       "      <td>VERB</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>How long has it been since you reviewed the ob...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d000.s000</td>\n",
       "      <td>d000.s000.t003</td>\n",
       "      <td>objective</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>objectives</td>\n",
       "      <td>How long has it been since you reviewed the ob...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d000.s000</td>\n",
       "      <td>d000.s000.t004</td>\n",
       "      <td>benefit</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>benefit</td>\n",
       "      <td>How long has it been since you reviewed the ob...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentence_id     instance_id      lemma   pos        word  \\\n",
       "0   d000.s000  d000.s000.t000       long   ADJ        long   \n",
       "1   d000.s000  d000.s000.t001         be  VERB        been   \n",
       "2   d000.s000  d000.s000.t002     review  VERB    reviewed   \n",
       "3   d000.s000  d000.s000.t003  objective  NOUN  objectives   \n",
       "4   d000.s000  d000.s000.t004    benefit  NOUN     benefit   \n",
       "\n",
       "                                       sentence_text  \n",
       "0  How long has it been since you reviewed the ob...  \n",
       "1  How long has it been since you reviewed the ob...  \n",
       "2  How long has it been since you reviewed the ob...  \n",
       "3  How long has it been since you reviewed the ob...  \n",
       "4  How long has it been since you reviewed the ob...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_xml_data(xml_file_path=''):\n",
    "    tree = ET.parse(xml_file_path)\n",
    "    root = tree.getroot()\n",
    "    data = []\n",
    "    for text in root.findall('text'):\n",
    "        for sentence in text.findall('sentence'):\n",
    "            sentence_id = sentence.get('id')\n",
    "            sentence_text = ' '.join([element.text for element in sentence])\n",
    "            for instance in sentence.findall('instance'):\n",
    "                instance_id = instance.get('id')\n",
    "                lemma = instance.get('lemma')\n",
    "                pos = instance.get('pos')\n",
    "                word = instance.text\n",
    "                data.append([sentence_id, instance_id, lemma, pos, word, sentence_text])\n",
    "    columns = ['sentence_id', 'instance_id', 'lemma', 'pos', 'word', 'sentence_text']\n",
    "    xml_data = pd.DataFrame(data, columns=columns)\n",
    "    return xml_data\n",
    "\n",
    "\n",
    "# Load xml training data from semcor\n",
    "semcor_training_xml = load_xml_data(semcor_training_xml_path)\n",
    "display(semcor_training_xml.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "236703e6-8065-40f0-99a0-36c35bc6c5af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instance_id</th>\n",
       "      <th>sense_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d000.s000.t000</td>\n",
       "      <td>long%3:00:02::</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d000.s000.t001</td>\n",
       "      <td>be%2:42:03::</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d000.s000.t002</td>\n",
       "      <td>review%2:31:00::</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d000.s000.t003</td>\n",
       "      <td>objective%1:09:00::</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d000.s000.t004</td>\n",
       "      <td>benefit%1:21:00::</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      instance_id             sense_id\n",
       "0  d000.s000.t000       long%3:00:02::\n",
       "1  d000.s000.t001         be%2:42:03::\n",
       "2  d000.s000.t002     review%2:31:00::\n",
       "3  d000.s000.t003  objective%1:09:00::\n",
       "4  d000.s000.t004    benefit%1:21:00::"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_gold_keys(gold_key_file_path=''):\n",
    "    gold_key_data = []\n",
    "    with open(gold_key_file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split()\n",
    "            instance_id = parts[0]\n",
    "            sense_id = parts[1]\n",
    "            gold_key_data.append([instance_id, sense_id])\n",
    "\n",
    "    # Create a DataFrame\n",
    "    gold_key_columns = ['instance_id', 'sense_id']\n",
    "    gold_key_df = pd.DataFrame(gold_key_data, columns=gold_key_columns)\n",
    "    return gold_key_df\n",
    "\n",
    "# Load gold key training data from semcor\n",
    "semcor_training_gk = load_gold_keys(semcor_training_gk_path)\n",
    "display(semcor_training_gk.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f0698de-a838-49d7-9084-1903a9ea0ff2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>instance_id</th>\n",
       "      <th>lemma</th>\n",
       "      <th>pos</th>\n",
       "      <th>word</th>\n",
       "      <th>sentence_text</th>\n",
       "      <th>sense_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d000.s000</td>\n",
       "      <td>d000.s000.t000</td>\n",
       "      <td>long</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>long</td>\n",
       "      <td>How long has it been since you reviewed the ob...</td>\n",
       "      <td>long%3:00:02::</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d000.s000</td>\n",
       "      <td>d000.s000.t001</td>\n",
       "      <td>be</td>\n",
       "      <td>VERB</td>\n",
       "      <td>been</td>\n",
       "      <td>How long has it been since you reviewed the ob...</td>\n",
       "      <td>be%2:42:03::</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d000.s000</td>\n",
       "      <td>d000.s000.t002</td>\n",
       "      <td>review</td>\n",
       "      <td>VERB</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>How long has it been since you reviewed the ob...</td>\n",
       "      <td>review%2:31:00::</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d000.s000</td>\n",
       "      <td>d000.s000.t003</td>\n",
       "      <td>objective</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>objectives</td>\n",
       "      <td>How long has it been since you reviewed the ob...</td>\n",
       "      <td>objective%1:09:00::</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d000.s000</td>\n",
       "      <td>d000.s000.t004</td>\n",
       "      <td>benefit</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>benefit</td>\n",
       "      <td>How long has it been since you reviewed the ob...</td>\n",
       "      <td>benefit%1:21:00::</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentence_id     instance_id      lemma   pos        word  \\\n",
       "0   d000.s000  d000.s000.t000       long   ADJ        long   \n",
       "1   d000.s000  d000.s000.t001         be  VERB        been   \n",
       "2   d000.s000  d000.s000.t002     review  VERB    reviewed   \n",
       "3   d000.s000  d000.s000.t003  objective  NOUN  objectives   \n",
       "4   d000.s000  d000.s000.t004    benefit  NOUN     benefit   \n",
       "\n",
       "                                       sentence_text             sense_id  \n",
       "0  How long has it been since you reviewed the ob...       long%3:00:02::  \n",
       "1  How long has it been since you reviewed the ob...         be%2:42:03::  \n",
       "2  How long has it been since you reviewed the ob...     review%2:31:00::  \n",
       "3  How long has it been since you reviewed the ob...  objective%1:09:00::  \n",
       "4  How long has it been since you reviewed the ob...    benefit%1:21:00::  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Merge data from two file\n",
    "semcor_training_merged = pd.merge(semcor_training_xml, semcor_training_gk, on='instance_id', how='inner')\n",
    "display(semcor_training_merged.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c08fc2cf-5830-49e1-8e19-b613a7e9b515",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemma</th>\n",
       "      <th>word</th>\n",
       "      <th>sentence_text</th>\n",
       "      <th>formatted_sense_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>long</td>\n",
       "      <td>long</td>\n",
       "      <td>How long has it been since you reviewed the ob...</td>\n",
       "      <td>long.a.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>be</td>\n",
       "      <td>been</td>\n",
       "      <td>How long has it been since you reviewed the ob...</td>\n",
       "      <td>be.v.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>review</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>How long has it been since you reviewed the ob...</td>\n",
       "      <td>review.v.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>objective</td>\n",
       "      <td>objectives</td>\n",
       "      <td>How long has it been since you reviewed the ob...</td>\n",
       "      <td>aim.n.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>benefit</td>\n",
       "      <td>benefit</td>\n",
       "      <td>How long has it been since you reviewed the ob...</td>\n",
       "      <td>benefit.n.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       lemma        word                                      sentence_text  \\\n",
       "0       long        long  How long has it been since you reviewed the ob...   \n",
       "1         be        been  How long has it been since you reviewed the ob...   \n",
       "2     review    reviewed  How long has it been since you reviewed the ob...   \n",
       "3  objective  objectives  How long has it been since you reviewed the ob...   \n",
       "4    benefit     benefit  How long has it been since you reviewed the ob...   \n",
       "\n",
       "  formatted_sense_id  \n",
       "0          long.a.01  \n",
       "1            be.v.01  \n",
       "2        review.v.01  \n",
       "3           aim.n.02  \n",
       "4       benefit.n.01  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sense_id_cache = {}\n",
    "def format_sense_id(sense_id):\n",
    "    if sense_id not in sense_id_cache:\n",
    "        sense_id_cache[sense_id] = wn.lemma_from_key(sense_id).synset().name()\n",
    "    return sense_id_cache[sense_id]\n",
    "    \n",
    "semcor_training_merged['formatted_sense_id'] = semcor_training_merged['sense_id'].apply(format_sense_id)\n",
    "\n",
    "# We keep those columns for now\n",
    "keys_to_keep = ['lemma', 'word', 'sentence_text', 'formatted_sense_id']\n",
    "semcor_training_merged = semcor_training_merged[keys_to_keep]\n",
    "\n",
    "display(semcor_training_merged.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49501b40-7528-4cfd-b84a-4331e86c02cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25916\n",
      "31\n",
      "229\n"
     ]
    }
   ],
   "source": [
    "unique_sense_ids = semcor_training_merged['formatted_sense_id'].unique()\n",
    "print(len(unique_sense_ids))\n",
    "sampled_ids = pd.Series(unique_sense_ids).sample(n=ball_num, random_state=1)\n",
    "print(len(sampled_ids))\n",
    "semcor_training = semcor_training_merged[semcor_training_merged['formatted_sense_id'].isin(sampled_ids)]\n",
    "print(len(semcor_training))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d5ac7d4-fa22-4409-9634-8de9c4d17d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting for the model choice\n",
    "models = {\n",
    "    \"BERT-Base\": \"bert-base-uncased\",\n",
    "    \"BERT-Large\": \"bert-large-uncased\",\n",
    "    \"BERT-Medium\": \"google/bert_uncased_L-8_H-512_A-8\",\n",
    "    \"BERT-Small\": \"google/bert_uncased_L-4_H-256_A-4\",\n",
    "    \"BERT-Mini\": \"google/bert_uncased_L-4_H-128_A-2\",\n",
    "    \"BERT-Tiny\": \"google/bert_uncased_L-2_H-128_A-2\"\n",
    "}\n",
    "\n",
    "# With our nball dimention 162, we choose bert small with 256 dimentions\n",
    "model_name = models[\"BERT-Small\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9f89f6d-8c57-446f-b15b-79febd5a6c69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data of nball embeddings:31\n",
      "The length after padding: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\annotated-transformer\\lib\\site-packages\\pandas\\core\\indexing.py:1667: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[key] = value\n"
     ]
    }
   ],
   "source": [
    "# Get sense index\n",
    "sense_labels = list(sampled_ids)\n",
    "sense_index_num = list(balls.keys())\n",
    "sense_index = {sense: idx for idx, sense in zip(sense_index_num, sense_labels)}\n",
    "semcor_training.loc[:,'sense_idx'] = semcor_training['formatted_sense_id'].map(sense_index)\n",
    "\n",
    "original_dim = len(balls[sense_index_num[0]]['center'])\n",
    "target_dim = 3  # Dimension of BERT-Small\n",
    "padding_size = target_dim - original_dim  # Adjust padding size based on the new dimension\n",
    "\n",
    "# Process each embedding\n",
    "padded_embeddings = []\n",
    "nball_radius = []\n",
    "scalling_factor = 1\n",
    "for index in sense_index_num:\n",
    "    # Pad the trimmed embedding to match the target dimension\n",
    "    padded_embedding = np.pad(balls[index]['center'], (0, padding_size), 'constant', constant_values=0)\n",
    "    # Add to the list of padded embeddings\n",
    "    padded_embeddings.append(scalling_factor*padded_embedding)\n",
    "    nball_radius.append(scalling_factor*balls[index]['radius'])\n",
    "\n",
    "sense_embeddings = torch.tensor(np.array(padded_embeddings), dtype=torch.float64)\n",
    "nball_radius = torch.tensor(np.array(nball_radius), dtype=torch.float64)\n",
    "\n",
    "print(f'Total data of nball embeddings:{len(sense_embeddings)}')\n",
    "print(f'The length after padding: {len(sense_embeddings[0])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95125cc4-24c5-4c30-85a1-01b1b7a0a97d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing sentences...\n",
      "Calculating word indices...\n",
      "Tokenizing finished!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemma</th>\n",
       "      <th>word</th>\n",
       "      <th>sentence_text</th>\n",
       "      <th>formatted_sense_id</th>\n",
       "      <th>sense_idx</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>word_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>hot</td>\n",
       "      <td>hot</td>\n",
       "      <td>Latest models serve hot meals at reasonable pr...</td>\n",
       "      <td>hot.a.01</td>\n",
       "      <td>11</td>\n",
       "      <td>[101, 6745, 4275, 3710, 2980, 12278, 2012, 960...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2702</th>\n",
       "      <td>partly</td>\n",
       "      <td>partly</td>\n",
       "      <td>The trouble was at least partly Juet 's doing .</td>\n",
       "      <td>partially.r.01</td>\n",
       "      <td>18</td>\n",
       "      <td>[101, 1996, 4390, 2001, 2012, 2560, 6576, 1841...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, ...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3176</th>\n",
       "      <td>experience</td>\n",
       "      <td>experience</td>\n",
       "      <td>On the one hand , the major European nations h...</td>\n",
       "      <td>experience.n.03</td>\n",
       "      <td>20</td>\n",
       "      <td>[101, 2006, 1996, 2028, 2192, 1010, 1996, 2350...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>66</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6558</th>\n",
       "      <td>enclosure</td>\n",
       "      <td>enclosure</td>\n",
       "      <td>Sometimes these servants wrote or dictated for...</td>\n",
       "      <td>enclosure.n.02</td>\n",
       "      <td>9</td>\n",
       "      <td>[101, 2823, 2122, 8858, 2626, 2030, 23826, 200...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8084</th>\n",
       "      <td>hot</td>\n",
       "      <td>hot</td>\n",
       "      <td>Eyes like hot honey , eyes that sizzled .</td>\n",
       "      <td>hot.a.01</td>\n",
       "      <td>11</td>\n",
       "      <td>[101, 2159, 2066, 2980, 6861, 1010, 2159, 2008...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           lemma        word  \\\n",
       "985          hot         hot   \n",
       "2702      partly      partly   \n",
       "3176  experience  experience   \n",
       "6558   enclosure   enclosure   \n",
       "8084         hot         hot   \n",
       "\n",
       "                                          sentence_text formatted_sense_id  \\\n",
       "985   Latest models serve hot meals at reasonable pr...           hot.a.01   \n",
       "2702    The trouble was at least partly Juet 's doing .     partially.r.01   \n",
       "3176  On the one hand , the major European nations h...    experience.n.03   \n",
       "6558  Sometimes these servants wrote or dictated for...     enclosure.n.02   \n",
       "8084          Eyes like hot honey , eyes that sizzled .           hot.a.01   \n",
       "\n",
       "      sense_idx                                          input_ids  \\\n",
       "985          11  [101, 6745, 4275, 3710, 2980, 12278, 2012, 960...   \n",
       "2702         18  [101, 1996, 4390, 2001, 2012, 2560, 6576, 1841...   \n",
       "3176         20  [101, 2006, 1996, 2028, 2192, 1010, 1996, 2350...   \n",
       "6558          9  [101, 2823, 2122, 8858, 2626, 2030, 23826, 200...   \n",
       "8084         11  [101, 2159, 2066, 2980, 6861, 1010, 2159, 2008...   \n",
       "\n",
       "                                         attention_mask  word_index  \n",
       "985   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...           4  \n",
       "2702  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, ...           6  \n",
       "3176  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...          66  \n",
       "6558  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...           8  \n",
       "8084  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, ...           3  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize the sentence\n",
    "# Initialize BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Could be problem here, as we always fid\n",
    "def find_word_index(sentence_ids, word):\n",
    "    word_tokens = tokenizer.tokenize(word)\n",
    "    word_ids = tokenizer.convert_tokens_to_ids(word_tokens)\n",
    "    for i in range(len(sentence_ids) - len(word_tokens) + 1):\n",
    "        if sentence_ids[i:i+len(word_tokens)].tolist() == word_ids:\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "def tokenize_data(df):\n",
    "    # Tokenize all sentences\n",
    "    print(\"Tokenizing sentences...\")\n",
    "    tokenized_data = tokenizer(list(df['sentence_text']), padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "    input_ids = tokenized_data['input_ids']\n",
    "    attention_masks = tokenized_data['attention_mask']\n",
    "\n",
    "    # Progress bar for calculating word indices\n",
    "    print(\"Calculating word indices...\")\n",
    "    # pbar = tqdm(total=df.shape[0], desc=\"Calculating word indices\")\n",
    "    word_indices = []\n",
    "    for sentence_ids, word in zip(input_ids, df['word']):\n",
    "        word_indices.append(find_word_index(sentence_ids, word))\n",
    "        # pbar.update(1)  # Update progress for each word index found\n",
    "\n",
    "    # print(f\"Length input_ids:{len(input_ids)}\\n Length attention_mask:{len(attention_masks)}\\n Length \\\n",
    "    # word_index:{len(word_indices)}\\n Length dataframe:{len(df)}\")\n",
    "    df.loc[:, 'input_ids'] = input_ids.tolist()\n",
    "    df.loc[:, 'attention_mask'] = attention_masks.tolist()\n",
    "    df.loc[:, 'word_index'] = word_indices\n",
    "\n",
    "    print('Tokenizing finished!')\n",
    "    # pbar.close()  # Close the progress bar after completion\n",
    "\n",
    "\n",
    "tokenize_data(semcor_training)\n",
    "display(semcor_training.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b76fa8a2-a5ac-4120-aa45-54f189c75144",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting, small batch size here so that do not out of cuda memory\n",
    "batch_size = 32\n",
    "\n",
    "# Create tensor dataset to speed learning\n",
    "# Convert lists of lists into tensors\n",
    "all_input_ids = torch.stack([torch.tensor(ids, dtype=torch.long) for ids in semcor_training['input_ids']])\n",
    "all_attention_masks = torch.stack([torch.tensor(mask, dtype=torch.long) for mask in semcor_training['attention_mask']])\n",
    "all_word_indices = torch.tensor(semcor_training['word_index'].tolist(), dtype=torch.long)\n",
    "all_senses = torch.tensor(semcor_training['sense_idx'].tolist(), dtype=torch.long)\n",
    "\n",
    "# Create a TensorDataset\n",
    "dataset = TensorDataset(all_input_ids, all_attention_masks, all_word_indices, all_senses)\n",
    "\n",
    "# Use DataLoader to handle batching\n",
    "dataloader = DataLoader(dataset, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "36018864-c690-428a-8d88-32a54ee91145",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1235, dtype=torch.float64)\n",
      "31\n"
     ]
    }
   ],
   "source": [
    "print(nball_radius.median())\n",
    "print(len(nball_radius))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "36c85936-1678-492f-a265-d1a4806bfaa2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/50: 100%|█████████████████████████████████████████████████████████████| 8/8 [00:01<00:00,  6.31it/s, loss=7.24]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 27.774158314282776, improve:-27.774158314282776\n",
      "distance:160.05121088027954, radius:18.194444444444446, differences:141.85676643583508\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/50: 100%|█████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 44.05it/s, loss=3.72]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Loss: 7.437861846845153, improve:20.336296467437624\n",
      "distance:73.268807888031, radius:18.194444444444446, differences:55.07436344358656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/50: 100%|█████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 43.87it/s, loss=2.49]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Loss: 3.6296523213751186, improve:3.8082095254700343\n",
      "distance:64.33247995376587, radius:18.194444444444446, differences:46.138035509321426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/50: 100%|█████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 44.13it/s, loss=1.51]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Loss: 2.3117747907551296, improve:1.317877530619989\n",
      "distance:49.003089904785156, radius:18.194444444444446, differences:30.80864546034071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/50: 100%|█████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 45.08it/s, loss=1.17]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Loss: 1.6859347023719664, improve:0.6258400883831632\n",
      "distance:45.12551212310791, radius:18.194444444444446, differences:26.931067678663464\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/50: 100%|████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 44.88it/s, loss=0.142]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Loss: 0.8923737839332646, improve:0.7935609184387018\n",
      "distance:34.51242238283157, radius:18.194444444444446, differences:16.317977938387127\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/50: 100%|█████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 45.74it/s, loss=1.03]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Loss: 0.7525221226148792, improve:0.13985166131838545\n",
      "distance:30.067126750946045, radius:18.194444444444446, differences:11.872682306501599\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/50: 100%|███████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 45.97it/s, loss=0.0601]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Loss: 0.5249439200620636, improve:0.22757820255281558\n",
      "distance:27.442778885364532, radius:18.194444444444446, differences:9.248334440920086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/50: 100%|████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 45.36it/s, loss=0.212]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Loss: 0.31464284767475675, improve:0.21030107238730683\n",
      "distance:20.464122414588928, radius:18.194444444444443, differences:2.2696779701444854\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/50: 100%|███████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 45.42it/s, loss=0.568]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10, Loss: 0.3221906928565076, improve:-0.007547845181750834\n",
      "distance:20.949612259864807, radius:18.194444444444443, differences:2.7551678154203643\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/50: 100%|████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 43.58it/s, loss=0.15]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11, Loss: 0.244926046097301, improve:0.07726464675920658\n",
      "distance:20.342198312282562, radius:18.194444444444446, differences:2.147753867838116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/50: 100%|█████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 44.34it/s, loss=0.00639]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12, Loss: 0.14782714252858006, improve:0.09709890356872095\n",
      "distance:18.574446499347687, radius:18.194444444444446, differences:0.38000205490324035\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/50: 100%|███████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 43.08it/s, loss=0.049]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13, Loss: 0.14291960643602802, improve:0.0049075360925520395\n",
      "distance:17.47724038362503, radius:18.194444444444443, differences:-0.7172040608194123\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/50: 100%|██████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 42.99it/s, loss=0.0217]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14, Loss: 0.12330030771446021, improve:0.019619298721567813\n",
      "distance:17.23867779970169, radius:18.194444444444443, differences:-0.9557666447427522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/50: 100%|██████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 43.70it/s, loss=0.0207]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15, Loss: 0.1203740900754646, improve:0.002926217638995615\n",
      "distance:16.280475914478302, radius:18.194444444444446, differences:-1.9139685299661444\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/50: 100%|███████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 43.12it/s, loss=0.153]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16, Loss: 0.10787436136204911, improve:0.012499728713415481\n",
      "distance:15.919270813465118, radius:18.194444444444443, differences:-2.2751736309793245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/50: 100%|██████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 43.36it/s, loss=0.0532]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17, Loss: 0.094685576428816, improve:0.01318878493323311\n",
      "distance:15.852753043174744, radius:18.19444444444445, differences:-2.3416914012697063\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/50: 100%|███████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 37.29it/s, loss=0.018]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18, Loss: 0.07104108967097061, improve:0.02364448675784539\n",
      "distance:14.934168696403503, radius:18.194444444444446, differences:-3.260275748040943\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/50: 100%|██████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 44.27it/s, loss=0.0291]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19, Loss: 0.06773449082903797, improve:0.0033065988419326425\n",
      "distance:14.915937542915344, radius:18.194444444444446, differences:-3.278506901529102\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/50: 100%|███████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 34.02it/s, loss=0.245]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20, Loss: 0.07506831161387864, improve:-0.0073338207848406695\n",
      "distance:14.191771149635315, radius:18.194444444444443, differences:-4.002673294809128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/50: 100%|███████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 44.43it/s, loss=0.054]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21, Loss: 0.06508046396431155, improve:0.009987847649567089\n",
      "distance:15.01462110877037, radius:18.194444444444446, differences:-3.179823335674076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/50: 100%|█████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 34.56it/s, loss=0.00949]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22, Loss: 0.04625046435477125, improve:0.0188299996095403\n",
      "distance:13.493551075458527, radius:18.194444444444446, differences:-4.70089336898592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/50: 100%|██████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 40.84it/s, loss=0.0517]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23, Loss: 0.04439749267709939, improve:0.001852971677671865\n",
      "distance:14.302472487092018, radius:18.194444444444443, differences:-3.8919719573524247\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/50: 100%|██████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 45.71it/s, loss=0.0124]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24, Loss: 0.03353376751361974, improve:0.010863725163479644\n",
      "distance:13.148218497633934, radius:18.194444444444446, differences:-5.046225946810512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/50: 100%|██████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 45.14it/s, loss=0.0116]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25, Loss: 0.048732732233104564, improve:-0.015198964719484821\n",
      "distance:14.024336993694305, radius:18.194444444444446, differences:-4.170107450750141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/50: 100%|██████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 45.54it/s, loss=0.0627]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26, Loss: 0.04569713907082391, improve:0.0030355931622806573\n",
      "distance:13.191233843564987, radius:18.194444444444446, differences:-5.003210600879459\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/50: 100%|██████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 45.60it/s, loss=0.0223]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27, Loss: 0.04626580171202871, improve:-0.0005686626412048021\n",
      "distance:13.629263669252396, radius:18.194444444444446, differences:-4.565180775192051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/50: 100%|███████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 44.81it/s, loss=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28, Loss: 0.038225071083711384, improve:0.008040730628317325\n",
      "distance:13.402493357658386, radius:18.19444444444445, differences:-4.791951086786064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/50: 100%|███████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 34.58it/s, loss=0.149]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29, Loss: 0.047706441453819845, improve:-0.00948137037010846\n",
      "distance:12.651562750339508, radius:18.194444444444446, differences:-5.542881694104938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/50: 100%|███████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 39.93it/s, loss=0.091]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30, Loss: 0.06680024250857425, improve:-0.019093801054754403\n",
      "distance:14.386580556631088, radius:18.194444444444446, differences:-3.807863887813358\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/50: 100%|██████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 42.55it/s, loss=0.0587]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31, Loss: 0.05117414321256929, improve:0.01562609929600496\n",
      "distance:14.068081736564636, radius:18.194444444444443, differences:-4.126362707879807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/50: 100%|█████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 33.67it/s, loss=0.00441]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32, Loss: 0.04418143308447935, improve:0.006992710128089934\n",
      "distance:12.86346447467804, radius:18.194444444444446, differences:-5.330979969766407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/50: 100%|████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 37.11it/s, loss=0.000157]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33, Loss: 0.023953528874333798, improve:0.020227904210145555\n",
      "distance:11.938200682401657, radius:18.194444444444446, differences:-6.256243762042789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/50: 100%|████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 37.22it/s, loss=0.000193]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34, Loss: 0.019020372252049823, improve:0.004933156622283975\n",
      "distance:11.790087461471558, radius:18.194444444444446, differences:-6.404356982972889\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/50: 100%|██████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 39.06it/s, loss=0.0222]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35, Loss: 0.022493676092665685, improve:-0.003473303840615862\n",
      "distance:12.08046606183052, radius:18.194444444444443, differences:-6.113978382613922\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/50: 100%|███████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 34.69it/s, loss=0.019]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36, Loss: 0.02249523437120121, improve:-1.5582785355267026e-06\n",
      "distance:11.65102243423462, radius:18.194444444444446, differences:-6.543422010209827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/50: 100%|██████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 43.37it/s, loss=0.0447]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37, Loss: 0.028165478740215973, improve:-0.005670244369014761\n",
      "distance:12.226624131202698, radius:18.194444444444446, differences:-5.967820313241749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/50: 100%|██████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 35.65it/s, loss=0.0636]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38, Loss: 0.024139089975124116, improve:0.004026388765091857\n",
      "distance:12.04725307226181, radius:18.194444444444446, differences:-6.147191372182636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/50: 100%|█████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 43.05it/s, loss=0.00839]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39, Loss: 0.024332482882035857, improve:-0.00019339290691174071\n",
      "distance:12.059675604104996, radius:18.194444444444446, differences:-6.134768840339451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/50: 100%|█████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 34.29it/s, loss=0.00123]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40, Loss: 0.021561109748646314, improve:0.002771373133389543\n",
      "distance:11.748284876346588, radius:18.194444444444446, differences:-6.446159568097858\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/50: 100%|█████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 38.85it/s, loss=0.00241]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41, Loss: 0.027171086681065746, improve:-0.0056099769324194325\n",
      "distance:12.038091614842415, radius:18.194444444444443, differences:-6.156352829602028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/50: 100%|█████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 40.83it/s, loss=0.00384]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42, Loss: 0.020048395707399354, improve:0.0071226909736663915\n",
      "distance:11.410722836852074, radius:18.194444444444446, differences:-6.783721607592373\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/50: 100%|███████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 36.36it/s, loss=0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43, Loss: 0.020137077165880556, improve:-8.86814584812011e-05\n",
      "distance:12.098889589309692, radius:18.194444444444446, differences:-6.095554855134754\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/50: 100%|███████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 36.99it/s, loss=0.133]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44, Loss: 0.03604886182215484, improve:-0.015911784656274283\n",
      "distance:12.0089191198349, radius:18.194444444444443, differences:-6.185525324609543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/50: 100%|█████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 39.82it/s, loss=0.00509]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45, Loss: 0.029102939362595642, improve:0.0069459224595591965\n",
      "distance:12.460154592990875, radius:18.194444444444446, differences:-5.734289851453571\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/50: 100%|█████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 35.64it/s, loss=0.00258]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46, Loss: 0.014413507769980145, improve:0.014689431592615498\n",
      "distance:11.408422231674194, radius:18.194444444444446, differences:-6.786022212770252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/50: 100%|█████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 42.91it/s, loss=0.00124]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47, Loss: 0.02197619804637136, improve:-0.007562690276391216\n",
      "distance:11.483596757054329, radius:18.194444444444443, differences:-6.710847687390114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/50: 100%|█████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 33.88it/s, loss=0.00806]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48, Loss: 0.01812447097667686, improve:0.0038517270696944994\n",
      "distance:11.479848027229309, radius:18.194444444444443, differences:-6.714596417215134\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/50: 100%|████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 42.76it/s, loss=0.02]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49, Loss: 0.020855560831372577, improve:-0.002731089854695716\n",
      "distance:12.022155493497849, radius:18.194444444444446, differences:-6.172288950946598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/50: 100%|█████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 36.88it/s, loss=0.00469]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50, Loss: 0.018154324748705347, improve:0.00270123608266723\n",
      "distance:11.452143862843513, radius:18.194444444444443, differences:-6.742300581600929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "# Loss\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import math\n",
    "\n",
    "# Set the device to CUDA if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# top_values, top_indices = torch.topk(nball_radius, 10)\n",
    "# top_indices = top_indices.to(device)\n",
    "\n",
    "def ball_inclusion_loss(word_embeddings, centers, radius, labels, epsilon=1e-7):\n",
    "    distances = torch.norm(word_embeddings - centers, dim=1)\n",
    "    # # Compute a binary mask for embeddings within the ball radius\n",
    "    # # print(f\"difference:{radius-distances}\")\n",
    "    within_ball = (distances <= radius).float()\n",
    "    within_ball = torch.clamp(within_ball, epsilon, 1 - epsilon)\n",
    "    loss = F.binary_cross_entropy(within_ball, labels.float(), reduction='mean')\n",
    "    return loss, distances\n",
    "\n",
    "def ball_inclution_loss_soft_margin(word_embeddings, centers, radius):\n",
    "    distances = torch.norm(word_embeddings - centers, dim=1)\n",
    "    soft_margin = torch.clamp(distances - radius, min=0)\n",
    "    loss = torch.mean(soft_margin ** 2)\n",
    "    return loss, distances\n",
    " \n",
    "\n",
    "class CustomBertModel(nn.Module):\n",
    "    def __init__(self, bert_model_name):\n",
    "        super(CustomBertModel, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained(bert_model_name)\n",
    "        self.projection = nn.Linear(self.bert.config.hidden_size, 3)\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        projected_hidden_states = self.projection(hidden_states)\n",
    "        return projected_hidden_states\n",
    "\n",
    "\n",
    "# Model setting\n",
    "# model = BertModel.from_pretrained(model_name).to(device)\n",
    "model = CustomBertModel(\"google/bert_uncased_L-2_H-128_A-2\").to(device)\n",
    "model.train() \n",
    "sense_embeddings = sense_embeddings.to(device)  # Move sense embeddings to GPU\n",
    "# loss_fn = ball_inclusion_loss\n",
    "# loss_fn = nn.MSELoss()\n",
    "loss_fn = ball_inclution_loss_soft_margin\n",
    "optimizer = optim.Adam([\n",
    "    {'params': model.parameters()}\n",
    "    # {'params': sense_embeddings},\n",
    "    # {'params': nball_radius}\n",
    "], lr=2e-3)\n",
    "\n",
    "scheduler = StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 50\n",
    "last_loss = 0\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    total_dis = 0\n",
    "    total_radius = 0\n",
    "    progress_bar = tqdm(dataloader, desc=f'Epoch {epoch+1}/{num_epochs}', leave=True, position=0)\n",
    "    for batch in progress_bar:\n",
    "        # Send batch data to the device (GPU)\n",
    "        batch_input_ids, batch_attention_masks, batch_word_indices, batch_sense_indices = [b.to(device) for b in batch]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        # outputs = model(input_ids=batch_input_ids, attention_mask=batch_attention_masks)\n",
    "        # hidden_states = outputs.last_hidden_state\n",
    "        hidden_states = model(input_ids=batch_input_ids, attention_mask=batch_attention_masks)\n",
    "        # Retrieve embeddings for specific word indices\n",
    "        word_embeddings = torch.stack([hidden_states[i, idx, :] for i, idx in enumerate(batch_word_indices)])\n",
    "        \n",
    "        # Retrieve the corresponding sense embeddings\n",
    "        center = sense_embeddings[batch_sense_indices].to(dtype=torch.float32).to(device)\n",
    "        radius = nball_radius[batch_sense_indices].to(device)\n",
    "\n",
    "        # Labels tensor indicating that embeddings should be inside the ball\n",
    "        labels = torch.ones(word_embeddings.size(0), device=device, requires_grad=True)\n",
    "        # print(\"word_embeddings requires grad:\", word_embeddings.requires_grad)\n",
    "        # print(\"centers requires grad:\", center.requires_grad)\n",
    "\n",
    "        # # You can also check for the outputs from the model\n",
    "        # print(\"Model labels requires grad:\", labels.requires_grad)\n",
    "\n",
    "        # Calculate loss\n",
    "        # loss, dis = loss_fn(word_embeddings, center, radius, labels, epsilon=1e-2)\n",
    "        loss, dis = loss_fn(word_embeddings, center, radius)\n",
    "        # loss = loss_fn(word_embeddings, center)\n",
    "        # dis = nn.MSELoss()(word_embeddings, center)\n",
    "        # distances = torch.norm(word_embeddings - center, dim=1)\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # for name, param in model.named_parameters():\n",
    "        #     if param.requires_grad and param.grad is not None:\n",
    "        #         print(f\"Gradient of {name}: {param.grad.abs().mean().item()}\")\n",
    "        #     elif param.requires_grad:\n",
    "        #         print(f\"No gradient for {name}\")\n",
    "\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        total_dis += dis.sum().item()\n",
    "        total_radius += radius.sum().item()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    \n",
    "    print(f'Epoch {epoch + 1}, Loss: {total_loss / len(dataloader)}, improve:{(last_loss - total_loss) / len(dataloader)}')\n",
    "    print(f'distance:{total_dis / len(dataloader)}, radius:{total_radius / len(dataloader)}, differences:{(total_dis-total_radius) / len(dataloader)}')\n",
    "    last_loss = total_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "efecfd4d-1d51-45cd-9a20-1db4c3f6b543",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluation: 100%|███████████████████████████████████████████████████████████████████████| 8/8 [00:00<00:00, 125.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Evaluation Loss: 9.305681765079498\n",
      "Accuracy: 42.27%\n"
     ]
    }
   ],
   "source": [
    "# Assuming that 'dataloader' for evaluation is available as eval_dataloader\n",
    "\n",
    "# Switch model to evaluation mode\n",
    "eval_dataloader = dataloader\n",
    "model.eval()\n",
    "\n",
    "# Store evaluation results\n",
    "eval_loss = 0\n",
    "accuracy = 0  # Placeholder for accuracy calculation\n",
    "\n",
    "not_correct = []\n",
    "# Disable gradient computation for evaluation to save memory and computations\n",
    "with torch.no_grad():\n",
    "    for batch in tqdm(eval_dataloader, desc=\"Evaluation\", leave=True, position=0):\n",
    "        # Send batch data to the device (GPU)\n",
    "        batch_input_ids, batch_attention_masks, batch_word_indices, batch_sense_indices = [b.to(device) for b in batch]\n",
    "\n",
    "        # Forward pass\n",
    "        hidden_states = model(input_ids=batch_input_ids, attention_mask=batch_attention_masks)\n",
    "        # hidden_states = outputs.last_hidden_state\n",
    "\n",
    "        # Retrieve embeddings for specific word indices\n",
    "        word_embeddings = torch.stack([hidden_states[i, idx, :] for i, idx in enumerate(batch_word_indices)])\n",
    "\n",
    "        # Retrieve the corresponding sense embeddings and radius\n",
    "        center = sense_embeddings[batch_sense_indices].to(dtype=torch.float32).to(device)\n",
    "        radius = nball_radius[batch_sense_indices].to(device)\n",
    "\n",
    "        # Labels tensor (usually true for all points in eval since you are evaluating the inclusion)\n",
    "        labels = torch.ones(word_embeddings.size(0), device=device)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss, dis = ball_inclusion_loss(word_embeddings, center, radius, labels)\n",
    "        eval_loss += loss.item()\n",
    "\n",
    "        # Optionally calculate accuracy or other metrics\n",
    "        # For instance, if accuracy is based on correct inclusions\n",
    "        predictions = (torch.norm(word_embeddings - center, dim=1) <= radius).float()\n",
    "        accuracy += (predictions == labels).float().mean().item()\n",
    "        incorrect_indices = batch_sense_indices[predictions != labels]\n",
    "        not_correct.extend(incorrect_indices.cpu().tolist())\n",
    "\n",
    "# Compute average loss and accuracy\n",
    "eval_loss /= len(eval_dataloader)\n",
    "accuracy /= len(eval_dataloader)\n",
    "\n",
    "print(f\"Average Evaluation Loss: {eval_loss}\")\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d4fc1496-a95e-43a2-a5cb-ec949ce52d1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3704, 0.3704, 0.1235, 0.1235, 0.1235, 0.1235, 0.3704, 0.3704, 0.1235,\n",
       "        0.1235, 0.1235, 0.1235, 0.3704, 0.3704, 0.1235, 0.1235, 0.1235, 0.1235,\n",
       "        0.1235, 0.1235, 0.1235, 0.1235], dtype=torch.float64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uni_not_correct = set(not_correct)\n",
    "nball_radius[list(uni_not_correct)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "187b5709-fc47-4b35-a92d-d16842d972db",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10.0000,  3.3333,  3.3333,  1.1111,  1.1111,  0.3704,  0.3704,  0.1235,\n",
       "         0.1235,  0.1235,  0.1235,  0.3704,  0.3704,  0.1235,  0.1235,  0.1235,\n",
       "         0.1235,  1.1111,  1.1111,  0.3704,  0.3704,  0.1235,  0.1235,  0.1235,\n",
       "         0.1235,  0.3704,  0.3704,  0.1235,  0.1235,  0.1235,  0.1235],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nball_radius"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e32988-8dac-4f46-a0ce-14b03937813e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (annotated-transformer)",
   "language": "python",
   "name": "annotated-transformer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
