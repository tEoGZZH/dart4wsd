{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6fccfbf-bc83-4bf8-8dd4-6d1e46d44661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the package\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from transformers import BertTokenizer, BertModel, AdamW\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import xml.etree.ElementTree as ET\n",
    "import nltk\n",
    "from nltk.corpus import wordnet as wn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de62996b-1afb-45aa-aa7d-f68d0de88f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings for loading training data\n",
    "semcor_training_xml_path = 'WSD_Evaluation_Framework/Training_Corpora/Semcor/semcor.data.xml'\n",
    "semcor_training_gk_path = 'WSD_Evaluation_Framework/Training_Corpora/Semcor/semcor.gold.key.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6206742c-c2b0-4a23-8f50-ee5ad5d39b5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>instance_id</th>\n",
       "      <th>lemma</th>\n",
       "      <th>pos</th>\n",
       "      <th>word</th>\n",
       "      <th>sentence_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d000.s000</td>\n",
       "      <td>d000.s000.t000</td>\n",
       "      <td>long</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>long</td>\n",
       "      <td>How long has it been since you reviewed the ob...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d000.s000</td>\n",
       "      <td>d000.s000.t001</td>\n",
       "      <td>be</td>\n",
       "      <td>VERB</td>\n",
       "      <td>been</td>\n",
       "      <td>How long has it been since you reviewed the ob...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d000.s000</td>\n",
       "      <td>d000.s000.t002</td>\n",
       "      <td>review</td>\n",
       "      <td>VERB</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>How long has it been since you reviewed the ob...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d000.s000</td>\n",
       "      <td>d000.s000.t003</td>\n",
       "      <td>objective</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>objectives</td>\n",
       "      <td>How long has it been since you reviewed the ob...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d000.s000</td>\n",
       "      <td>d000.s000.t004</td>\n",
       "      <td>benefit</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>benefit</td>\n",
       "      <td>How long has it been since you reviewed the ob...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentence_id     instance_id      lemma   pos        word  \\\n",
       "0   d000.s000  d000.s000.t000       long   ADJ        long   \n",
       "1   d000.s000  d000.s000.t001         be  VERB        been   \n",
       "2   d000.s000  d000.s000.t002     review  VERB    reviewed   \n",
       "3   d000.s000  d000.s000.t003  objective  NOUN  objectives   \n",
       "4   d000.s000  d000.s000.t004    benefit  NOUN     benefit   \n",
       "\n",
       "                                       sentence_text  \n",
       "0  How long has it been since you reviewed the ob...  \n",
       "1  How long has it been since you reviewed the ob...  \n",
       "2  How long has it been since you reviewed the ob...  \n",
       "3  How long has it been since you reviewed the ob...  \n",
       "4  How long has it been since you reviewed the ob...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_xml_data(xml_file_path=''):\n",
    "    tree = ET.parse(xml_file_path)\n",
    "    root = tree.getroot()\n",
    "    data = []\n",
    "    for text in root.findall('text'):\n",
    "        for sentence in text.findall('sentence'):\n",
    "            sentence_id = sentence.get('id')\n",
    "            sentence_text = ' '.join([element.text for element in sentence])\n",
    "            for instance in sentence.findall('instance'):\n",
    "                instance_id = instance.get('id')\n",
    "                lemma = instance.get('lemma')\n",
    "                pos = instance.get('pos')\n",
    "                word = instance.text\n",
    "                data.append([sentence_id, instance_id, lemma, pos, word, sentence_text])\n",
    "    columns = ['sentence_id', 'instance_id', 'lemma', 'pos', 'word', 'sentence_text']\n",
    "    xml_data = pd.DataFrame(data, columns=columns)\n",
    "    return xml_data\n",
    "\n",
    "\n",
    "# Load xml training data from semcor\n",
    "semcor_training_xml = load_xml_data(semcor_training_xml_path)\n",
    "display(semcor_training_xml.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81cfe46f-f4a2-4230-975d-18597708fc65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>instance_id</th>\n",
       "      <th>sense_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d000.s000.t000</td>\n",
       "      <td>long%3:00:02::</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d000.s000.t001</td>\n",
       "      <td>be%2:42:03::</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d000.s000.t002</td>\n",
       "      <td>review%2:31:00::</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d000.s000.t003</td>\n",
       "      <td>objective%1:09:00::</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d000.s000.t004</td>\n",
       "      <td>benefit%1:21:00::</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      instance_id             sense_id\n",
       "0  d000.s000.t000       long%3:00:02::\n",
       "1  d000.s000.t001         be%2:42:03::\n",
       "2  d000.s000.t002     review%2:31:00::\n",
       "3  d000.s000.t003  objective%1:09:00::\n",
       "4  d000.s000.t004    benefit%1:21:00::"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def load_gold_keys(gold_key_file_path=''):\n",
    "    gold_key_data = []\n",
    "    with open(gold_key_file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split()\n",
    "            instance_id = parts[0]\n",
    "            sense_id = parts[1]\n",
    "            gold_key_data.append([instance_id, sense_id])\n",
    "\n",
    "    # Create a DataFrame\n",
    "    gold_key_columns = ['instance_id', 'sense_id']\n",
    "    gold_key_df = pd.DataFrame(gold_key_data, columns=gold_key_columns)\n",
    "    return gold_key_df\n",
    "\n",
    "# Load gold key training data from semcor\n",
    "semcor_training_gk = load_gold_keys(semcor_training_gk_path)\n",
    "display(semcor_training_gk.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54816649-bb91-4fe7-920d-31889b6abccc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_id</th>\n",
       "      <th>instance_id</th>\n",
       "      <th>lemma</th>\n",
       "      <th>pos</th>\n",
       "      <th>word</th>\n",
       "      <th>sentence_text</th>\n",
       "      <th>sense_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>d000.s000</td>\n",
       "      <td>d000.s000.t000</td>\n",
       "      <td>long</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>long</td>\n",
       "      <td>How long has it been since you reviewed the ob...</td>\n",
       "      <td>long%3:00:02::</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>d000.s000</td>\n",
       "      <td>d000.s000.t001</td>\n",
       "      <td>be</td>\n",
       "      <td>VERB</td>\n",
       "      <td>been</td>\n",
       "      <td>How long has it been since you reviewed the ob...</td>\n",
       "      <td>be%2:42:03::</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>d000.s000</td>\n",
       "      <td>d000.s000.t002</td>\n",
       "      <td>review</td>\n",
       "      <td>VERB</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>How long has it been since you reviewed the ob...</td>\n",
       "      <td>review%2:31:00::</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>d000.s000</td>\n",
       "      <td>d000.s000.t003</td>\n",
       "      <td>objective</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>objectives</td>\n",
       "      <td>How long has it been since you reviewed the ob...</td>\n",
       "      <td>objective%1:09:00::</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>d000.s000</td>\n",
       "      <td>d000.s000.t004</td>\n",
       "      <td>benefit</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>benefit</td>\n",
       "      <td>How long has it been since you reviewed the ob...</td>\n",
       "      <td>benefit%1:21:00::</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  sentence_id     instance_id      lemma   pos        word  \\\n",
       "0   d000.s000  d000.s000.t000       long   ADJ        long   \n",
       "1   d000.s000  d000.s000.t001         be  VERB        been   \n",
       "2   d000.s000  d000.s000.t002     review  VERB    reviewed   \n",
       "3   d000.s000  d000.s000.t003  objective  NOUN  objectives   \n",
       "4   d000.s000  d000.s000.t004    benefit  NOUN     benefit   \n",
       "\n",
       "                                       sentence_text             sense_id  \n",
       "0  How long has it been since you reviewed the ob...       long%3:00:02::  \n",
       "1  How long has it been since you reviewed the ob...         be%2:42:03::  \n",
       "2  How long has it been since you reviewed the ob...     review%2:31:00::  \n",
       "3  How long has it been since you reviewed the ob...  objective%1:09:00::  \n",
       "4  How long has it been since you reviewed the ob...    benefit%1:21:00::  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Merge data from two file\n",
    "semcor_training_merged = pd.merge(semcor_training_xml, semcor_training_gk, on='instance_id', how='inner')\n",
    "display(semcor_training_merged.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a24676a-be13-4190-8189-5834f156512a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemma</th>\n",
       "      <th>word</th>\n",
       "      <th>sentence_text</th>\n",
       "      <th>formatted_sense_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>long</td>\n",
       "      <td>long</td>\n",
       "      <td>How long has it been since you reviewed the ob...</td>\n",
       "      <td>long.a.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>be</td>\n",
       "      <td>been</td>\n",
       "      <td>How long has it been since you reviewed the ob...</td>\n",
       "      <td>be.v.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>review</td>\n",
       "      <td>reviewed</td>\n",
       "      <td>How long has it been since you reviewed the ob...</td>\n",
       "      <td>review.v.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>objective</td>\n",
       "      <td>objectives</td>\n",
       "      <td>How long has it been since you reviewed the ob...</td>\n",
       "      <td>aim.n.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>benefit</td>\n",
       "      <td>benefit</td>\n",
       "      <td>How long has it been since you reviewed the ob...</td>\n",
       "      <td>benefit.n.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       lemma        word                                      sentence_text  \\\n",
       "0       long        long  How long has it been since you reviewed the ob...   \n",
       "1         be        been  How long has it been since you reviewed the ob...   \n",
       "2     review    reviewed  How long has it been since you reviewed the ob...   \n",
       "3  objective  objectives  How long has it been since you reviewed the ob...   \n",
       "4    benefit     benefit  How long has it been since you reviewed the ob...   \n",
       "\n",
       "  formatted_sense_id  \n",
       "0          long.a.01  \n",
       "1            be.v.01  \n",
       "2        review.v.01  \n",
       "3           aim.n.02  \n",
       "4       benefit.n.01  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sense_id_cache = {}\n",
    "def format_sense_id(sense_id):\n",
    "    if sense_id not in sense_id_cache:\n",
    "        sense_id_cache[sense_id] = wn.lemma_from_key(sense_id).synset().name()\n",
    "    return sense_id_cache[sense_id]\n",
    "    \n",
    "semcor_training_merged['formatted_sense_id'] = semcor_training_merged['sense_id'].apply(format_sense_id)\n",
    "\n",
    "# We keep those columns for now\n",
    "keys_to_keep = ['lemma', 'word', 'sentence_text', 'formatted_sense_id']\n",
    "semcor_training_merged = semcor_training_merged[keys_to_keep]\n",
    "\n",
    "display(semcor_training_merged.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fb72ad8-791d-43e8-807d-c6d678fb8b9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading balls....\n",
      "8691  balls are loaded\n",
      "\n",
      "The length of nball embeddings: [159]\n"
     ]
    }
   ],
   "source": [
    "# Setting to load nball embeddings\n",
    "nball_small_path = 'training_set/nballSmall.txt'\n",
    "\n",
    "def load_ball_embeddings(bFile):\n",
    "    print(\"loading balls....\")\n",
    "    bdic=dict()\n",
    "    with open(bFile, 'r') as w2v:\n",
    "        for line in w2v.readlines():\n",
    "            wlst = line.strip().split()\n",
    "            bdic[wlst[0]] = list(map(float, wlst[1:]))\n",
    "    print(len(bdic),' balls are loaded\\n')\n",
    "    return bdic\n",
    "\n",
    "\n",
    "# Load ball embeddings\n",
    "nball_small = load_ball_embeddings(nball_small_path)\n",
    "\n",
    "# Check the length of the nball embeddings\n",
    "nball_len =[]\n",
    "for i, (key, value) in enumerate(nball_small.items()):\n",
    "    value_len = len(nball_small[key])\n",
    "    if value_len not in nball_len:\n",
    "        nball_len.append(value_len)\n",
    "print(f'The length of nball embeddings: {nball_len}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0418ece5-333d-4970-befb-c3de1f949f92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original length of the training set:226036\n",
      "Actual data used of the training set:20032\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemma</th>\n",
       "      <th>word</th>\n",
       "      <th>sentence_text</th>\n",
       "      <th>formatted_sense_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>program</td>\n",
       "      <td>program</td>\n",
       "      <td>How long has it been since you reviewed the ob...</td>\n",
       "      <td>program.n.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>program</td>\n",
       "      <td>program</td>\n",
       "      <td>Have you permitted it to become a giveaway pro...</td>\n",
       "      <td>program.n.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>program</td>\n",
       "      <td>program</td>\n",
       "      <td>What effort do you make to assess results of y...</td>\n",
       "      <td>program.n.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>use</td>\n",
       "      <td>using</td>\n",
       "      <td>Are you using the most economical printing met...</td>\n",
       "      <td>use.v.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>eating</td>\n",
       "      <td>eating</td>\n",
       "      <td>When improvements are recommended in working c...</td>\n",
       "      <td>eating.n.01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      lemma     word                                      sentence_text  \\\n",
       "6   program  program  How long has it been since you reviewed the ob...   \n",
       "10  program  program  Have you permitted it to become a giveaway pro...   \n",
       "24  program  program  What effort do you make to assess results of y...   \n",
       "46      use    using  Are you using the most economical printing met...   \n",
       "80   eating   eating  When improvements are recommended in working c...   \n",
       "\n",
       "   formatted_sense_id  \n",
       "6        program.n.02  \n",
       "10       program.n.02  \n",
       "24       program.n.02  \n",
       "46           use.v.01  \n",
       "80        eating.n.01  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For now, we train on exist nball\n",
    "\n",
    "print(f'Original length of the training set:{len(semcor_training_merged)}')\n",
    "semcor_training = semcor_training_merged[semcor_training_merged['formatted_sense_id'].isin(nball_small.keys())].copy()\n",
    "print(f'Actual data used of the training set:{len(semcor_training)}')\n",
    "\n",
    "display(semcor_training.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8ba3d3b-297f-44c9-b023-a488fe2b133c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting for the model choice\n",
    "models = {\n",
    "    \"BERT-Base\": \"bert-base-uncased\",\n",
    "    \"BERT-Large\": \"bert-large-uncased\",\n",
    "    \"BERT-Medium\": \"google/bert_uncased_L-8_H-512_A-8\",\n",
    "    \"BERT-Small\": \"google/bert_uncased_L-4_H-256_A-4\",\n",
    "    \"BERT-Mini\": \"google/bert_uncased_L-4_H-128_A-2\",\n",
    "    \"BERT-Tiny\": \"google/bert_uncased_L-2_H-128_A-2\"\n",
    "}\n",
    "\n",
    "# With our nball dimention 159, we choose bert small with 256 dimentions\n",
    "model_name = models[\"BERT-Small\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc2b6c91-325a-4abb-87d1-b51fe728d13e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total data of nball embeddings:8691\n",
      "The length after padding: 256\n"
     ]
    }
   ],
   "source": [
    "# Get sense index\n",
    "sense_labels = list(nball_small.keys())\n",
    "sense_index = {sense: idx for idx, sense in enumerate(sense_labels)}\n",
    "semcor_training.loc[:,'sense_idx'] = semcor_training['formatted_sense_id'].map(sense_index)\n",
    "\n",
    "# Padding the embeddings\n",
    "original_dim = len(nball_small[sense_labels[0]])\n",
    "target_dim = 256  # Dimension of BERT-Small\n",
    "padding_size = target_dim - original_dim\n",
    "\n",
    "# Pad each embedding to match the target dimension\n",
    "padded_embeddings = [np.pad(nball_small[label], (0, padding_size), 'constant', constant_values=0) for label in sense_labels]\n",
    "# Convert to tensor\n",
    "# The last one is too large \n",
    "sense_embeddings = torch.tensor(np.array(padded_embeddings), dtype=torch.float64)\n",
    "\n",
    "print(f'Total data of nball embeddings:{len(sense_embeddings)}')\n",
    "print(f'The length after padding: {len(sense_embeddings[0])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b989814-2f4d-4bd8-85fc-50d0e852b6c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing sentences...\n",
      "Calculating word indices...\n",
      "Tokenizing finished!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lemma</th>\n",
       "      <th>word</th>\n",
       "      <th>sentence_text</th>\n",
       "      <th>formatted_sense_id</th>\n",
       "      <th>sense_idx</th>\n",
       "      <th>input_ids</th>\n",
       "      <th>attention_mask</th>\n",
       "      <th>word_index</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>program</td>\n",
       "      <td>program</td>\n",
       "      <td>How long has it been since you reviewed the ob...</td>\n",
       "      <td>program.n.02</td>\n",
       "      <td>2382</td>\n",
       "      <td>[101, 2129, 2146, 2038, 2009, 2042, 2144, 2017...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>program</td>\n",
       "      <td>program</td>\n",
       "      <td>Have you permitted it to become a giveaway pro...</td>\n",
       "      <td>program.n.02</td>\n",
       "      <td>2382</td>\n",
       "      <td>[101, 2031, 2017, 7936, 2009, 2000, 2468, 1037...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>program</td>\n",
       "      <td>program</td>\n",
       "      <td>What effort do you make to assess results of y...</td>\n",
       "      <td>program.n.02</td>\n",
       "      <td>2382</td>\n",
       "      <td>[101, 2054, 3947, 2079, 2017, 2191, 2000, 1435...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, ...</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>use</td>\n",
       "      <td>using</td>\n",
       "      <td>Are you using the most economical printing met...</td>\n",
       "      <td>use.v.01</td>\n",
       "      <td>7420</td>\n",
       "      <td>[101, 2024, 2017, 2478, 1996, 2087, 21791, 802...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>eating</td>\n",
       "      <td>eating</td>\n",
       "      <td>When improvements are recommended in working c...</td>\n",
       "      <td>eating.n.01</td>\n",
       "      <td>3194</td>\n",
       "      <td>[101, 2043, 8377, 2024, 6749, 1999, 2551, 3785...</td>\n",
       "      <td>[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      lemma     word                                      sentence_text  \\\n",
       "6   program  program  How long has it been since you reviewed the ob...   \n",
       "10  program  program  Have you permitted it to become a giveaway pro...   \n",
       "24  program  program  What effort do you make to assess results of y...   \n",
       "46      use    using  Are you using the most economical printing met...   \n",
       "80   eating   eating  When improvements are recommended in working c...   \n",
       "\n",
       "   formatted_sense_id  sense_idx  \\\n",
       "6        program.n.02       2382   \n",
       "10       program.n.02       2382   \n",
       "24       program.n.02       2382   \n",
       "46           use.v.01       7420   \n",
       "80        eating.n.01       3194   \n",
       "\n",
       "                                            input_ids  \\\n",
       "6   [101, 2129, 2146, 2038, 2009, 2042, 2144, 2017...   \n",
       "10  [101, 2031, 2017, 7936, 2009, 2000, 2468, 1037...   \n",
       "24  [101, 2054, 3947, 2079, 2017, 2191, 2000, 1435...   \n",
       "46  [101, 2024, 2017, 2478, 1996, 2087, 21791, 802...   \n",
       "80  [101, 2043, 8377, 2024, 6749, 1999, 2551, 3785...   \n",
       "\n",
       "                                       attention_mask  word_index  \n",
       "6   [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...          16  \n",
       "10  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...          10  \n",
       "24  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, ...          11  \n",
       "46  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...           3  \n",
       "80  [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...          16  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tokenize the sentence\n",
    "# Initialize BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Could be problem here, as we always fid\n",
    "def find_word_index(sentence_ids, word):\n",
    "    word_tokens = tokenizer.tokenize(word)\n",
    "    word_ids = tokenizer.convert_tokens_to_ids(word_tokens)\n",
    "    for i in range(len(sentence_ids) - len(word_tokens) + 1):\n",
    "        if sentence_ids[i:i+len(word_tokens)].tolist() == word_ids:\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "def tokenize_data(df):\n",
    "    # Tokenize all sentences\n",
    "    print(\"Tokenizing sentences...\")\n",
    "    tokenized_data = tokenizer(list(df['sentence_text']), padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "    input_ids = tokenized_data['input_ids']\n",
    "    attention_masks = tokenized_data['attention_mask']\n",
    "\n",
    "    # Progress bar for calculating word indices\n",
    "    print(\"Calculating word indices...\")\n",
    "    # pbar = tqdm(total=df.shape[0], desc=\"Calculating word indices\")\n",
    "    word_indices = []\n",
    "    for sentence_ids, word in zip(input_ids, df['word']):\n",
    "        word_indices.append(find_word_index(sentence_ids, word))\n",
    "        # pbar.update(1)  # Update progress for each word index found\n",
    "\n",
    "    # print(f\"Length input_ids:{len(input_ids)}\\n Length attention_mask:{len(attention_masks)}\\n Length \\\n",
    "    # word_index:{len(word_indices)}\\n Length dataframe:{len(df)}\")\n",
    "    df.loc[:, 'input_ids'] = input_ids.tolist()\n",
    "    df.loc[:, 'attention_mask'] = attention_masks.tolist()\n",
    "    df.loc[:, 'word_index'] = word_indices\n",
    "\n",
    "    print('Tokenizing finished!')\n",
    "    # pbar.close()  # Close the progress bar after completion\n",
    "\n",
    "\n",
    "tokenize_data(semcor_training)\n",
    "display(semcor_training.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "42ad9684-56f1-4f1d-a44d-fefd70505cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting, small batch size here so that do not out of cuda memory\n",
    "batch_size = 2\n",
    "\n",
    "# Create tensor dataset to speed learning\n",
    "# Convert lists of lists into tensors\n",
    "all_input_ids = torch.stack([torch.tensor(ids, dtype=torch.long) for ids in semcor_training['input_ids']])\n",
    "all_attention_masks = torch.stack([torch.tensor(mask, dtype=torch.long) for mask in semcor_training['attention_mask']])\n",
    "all_word_indices = torch.tensor(semcor_training['word_index'].tolist(), dtype=torch.long)\n",
    "all_senses = torch.tensor(semcor_training['sense_idx'].tolist(), dtype=torch.long)\n",
    "\n",
    "# Create a TensorDataset\n",
    "dataset = TensorDataset(all_input_ids, all_attention_masks, all_word_indices, all_senses)\n",
    "\n",
    "# Use DataLoader to handle batching\n",
    "dataloader = DataLoader(dataset, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "210d066b-a49b-483c-bb7d-c330ce26913f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Cosine Similarity Loss\n",
    "def cosine_similarity_loss(embeddings1, embeddings2):\n",
    "    # Cosine similarity returns a value between -1 and 1, where 1 means identical\n",
    "    cosine_sim = F.cosine_similarity(embeddings1, embeddings2, dim=1)\n",
    "    print(cosine_sim)\n",
    "    # We subtract from 1 to convert similarity to loss: 0 means identical, 2 means totally opposite\n",
    "    return (1 - cosine_sim).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c2946108-e8f2-43c7-8a8d-54af6f036552",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Cosine similarity: 0.0\n",
      "Word embedding norm: 14.00044059753418\n",
      "Target embedding norm: inf\n",
      "Any NaN in word embeddings? tensor(False, device='cuda:0')\n",
      "Any NaN in target embeddings? tensor(False, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Model setting\n",
    "model = BertModel.from_pretrained(model_name).to(device)\n",
    "model.train() \n",
    "sense_embeddings = sense_embeddings.to(device)  # Move sense embeddings to GPU\n",
    "loss_fn = cosine_similarity_loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Fetch one batch of data\n",
    "data_iter = iter(dataloader)\n",
    "batch = next(data_iter)\n",
    "batch_input_ids, batch_attention_masks, batch_word_indices, batch_sense_indices = [b.to(device) for b in batch]\n",
    "\n",
    "# Forward pass to get outputs\n",
    "model.eval()  # Set the model to evaluation mode to disable dropout\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids=batch_input_ids, attention_mask=batch_attention_masks)\n",
    "    hidden_states = outputs.last_hidden_state\n",
    "\n",
    "# Retrieve embeddings for specific word indices\n",
    "word_embeddings = torch.stack([hidden_states[i, idx, :] for i, idx in enumerate(batch_word_indices)])\n",
    "\n",
    "# Retrieve the corresponding sense embeddings\n",
    "target_embeddings = sense_embeddings[batch_sense_indices]\n",
    "\n",
    "# Select one pair of embeddings\n",
    "example_word_embedding = word_embeddings[0]\n",
    "example_target_embedding = target_embeddings[0]\n",
    "\n",
    "# Calculate cosine similarity manually\n",
    "cosine_similarity = F.cosine_similarity(example_word_embedding.unsqueeze(0), example_target_embedding.unsqueeze(0))\n",
    "\n",
    "print(f'Cosine similarity: {cosine_similarity.item()}')\n",
    "print(f'Word embedding norm: {torch.norm(example_word_embedding)}')\n",
    "print(f'Target embedding norm: {torch.norm(example_target_embedding)}')\n",
    "\n",
    "# Optionally check for any NaN values\n",
    "print(\"Any NaN in word embeddings?\", torch.isnan(word_embeddings).any())\n",
    "print(\"Any NaN in target embeddings?\", torch.isnan(target_embeddings).any())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "08af0cd9-057e-454e-9b2c-0b0a52782c29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7704, 7213], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(batch_sense_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "697af050-fff4-46de-ab0b-8684d20f4388",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are tensors exactly equal? True\n"
     ]
    }
   ],
   "source": [
    "comparison = torch.equal(sense_embeddings[7704], example_target_embedding)\n",
    "print(\"Are tensors exactly equal?\", comparison)\n",
    "differences = sense_embeddings[7704] != example_target_embedding\n",
    "\n",
    "# Find indices where the values differ\n",
    "diff_indices = differences.nonzero(as_tuple=True)\n",
    "\n",
    "# Print out the differing elements\n",
    "for idx in diff_indices[0]:\n",
    "    print(f\"Index: {idx}, sense_embeddings[7226]: {sense_embeddings[7226][idx]}, example_target_embedding: {example_target_embedding[idx]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e43c0f24-2e48-4c32-bef2-6191e6e9b18c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-2.5823e-03,  6.9978e-04, -3.1382e-03,  1.2778e-03, -7.1062e-04,\n",
      "        -1.2048e-04, -2.2161e-03,  2.6910e-03, -3.3114e-03,  2.3125e-03,\n",
      "         1.7366e-04,  1.9518e-03, -4.4959e-03,  1.5646e-03,  2.5900e-03,\n",
      "         5.1694e-05,  2.8415e-03, -1.2466e-03, -3.5970e-03, -4.8595e-04,\n",
      "        -1.6719e-03,  1.1232e-03,  1.2293e-03,  2.7305e-03,  2.1141e-03,\n",
      "        -4.3246e-03,  1.5143e-04, -1.4513e-03,  1.2121e-03, -4.5956e-03,\n",
      "         1.2173e-02,  3.4685e-03,  1.3427e-04, -1.1819e-03,  1.0880e-03,\n",
      "         2.2221e-03,  1.0810e-03,  2.4053e-03, -7.7180e-04, -2.7204e-03,\n",
      "        -1.2946e-04,  1.8160e-04, -8.9300e-04, -2.2255e-04, -7.2533e-04,\n",
      "         3.2039e-03,  2.8126e-03, -2.6422e-04, -1.5601e-03,  1.6650e-03,\n",
      "         2.1219e-03,  1.5529e-01,  1.9290e-03,  1.9290e-03,  1.9290e-03,\n",
      "         1.9290e-03,  1.9290e-03,  9.8767e-02, -9.8767e-02, -9.8767e-02,\n",
      "        -9.8767e-02, -9.8767e-02, -9.8767e-02, -9.8767e-02, -9.8767e-02,\n",
      "        -9.8767e-02, -9.8767e-02, -9.8767e-02, -9.8767e-02, -9.8767e-02,\n",
      "        -9.8767e-02, -9.8767e-02, -9.8767e-02, -9.8767e-02, -9.8767e-02,\n",
      "        -9.8767e-02, -9.8767e-02, -9.8767e-02, -9.8767e-02, -9.8767e-02,\n",
      "        -9.8767e-02, -9.8767e-02, -9.8767e-02, -9.8767e-02, -9.8767e-02,\n",
      "        -9.8767e-02, -9.8767e-02, -9.8767e-02, -9.8767e-02, -9.8767e-02,\n",
      "        -9.8767e-02, -9.8767e-02, -9.8767e-02, -9.8767e-02, -9.8767e-02,\n",
      "        -9.8767e-02, -9.8767e-02, -9.8767e-02, -9.8767e-02, -9.8767e-02,\n",
      "        -9.8767e-02, -9.8767e-02, -9.8767e-02, -9.8767e-02, -9.8767e-02,\n",
      "        -9.8767e-02, -9.8767e-02, -9.8767e-02, -9.8767e-02, -9.8767e-02,\n",
      "        -9.8767e-02, -9.8767e-02, -9.8767e-02, -9.8767e-02, -9.8767e-02,\n",
      "        -9.8767e-02, -9.8767e-02, -9.8767e-02, -9.8767e-02, -9.8767e-02,\n",
      "        -9.8767e-02, -9.8767e-02, -9.8767e-02, -9.8767e-02, -9.8767e-02,\n",
      "        -9.8767e-02, -9.8767e-02, -9.8767e-02, -9.8767e-02, -9.8767e-02,\n",
      "        -9.8767e-02, -9.8767e-02, -9.8767e-02, -9.8767e-02, -9.8767e-02,\n",
      "        -9.8767e-02, -9.8767e-02, -9.8767e-02, -9.8767e-02, -9.8767e-02,\n",
      "        -9.8767e-02, -9.8767e-02, -9.8767e-02, -9.8767e-02, -9.8767e-02,\n",
      "        -9.8767e-02, -9.8767e-02, -9.8767e-02, -9.8767e-02, -9.8767e-02,\n",
      "        -9.8767e-02, -9.8767e-02, -9.8767e-02, -9.8767e-02, -9.8767e-02,\n",
      "        -9.8767e-02, -9.8767e-02, 1.1838e+174, 1.1838e-126,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00,\n",
      "         0.0000e+00], device='cuda:0', dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(sense_embeddings[7704])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "314467dd-169f-4953-8b92-8b6dd43dfb74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(inf, device='cuda:0', dtype=torch.float64)\n",
      "tensor([1.1838e+174, 1.1838e-126], dtype=torch.float64)\n",
      "tensor(inf, dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(torch.norm(sense_embeddings[7226]))\n",
    "print(torch.tensor([1.1838e+174, 1.1838e-126], dtype=torch.float64))\n",
    "print(torch.norm(torch.tensor([1.1838e+174, 1.1838e-126], dtype=torch.float64)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8dc18284-a4d3-4228-956a-653d1c68cee9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Are tensors exactly equal? False\n"
     ]
    }
   ],
   "source": [
    "comparison = torch.equal(sense_embeddings[7226], example_target_embedding)\n",
    "print(\"Are tensors exactly equal?\", comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce1c6adc-1478-4616-80c6-403945396a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pass.v.05\n",
      "7367\n",
      "4.074568017658608e+159\n"
     ]
    }
   ],
   "source": [
    "print(sense_labels[7367])\n",
    "print(sense_index['pass.v.05'])\n",
    "print(nball_small['pass.v.05'][157])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a5701f9c-a8ae-4f4f-8e95-b84dcac19689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:   8%|████▎                                                    | 755/10016 [00:17<03:37, 42.67it/s, loss=nan]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 43\u001b[0m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[0;32m     42\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m---> 43\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     45\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     46\u001b[0m progress_bar\u001b[38;5;241m.\u001b[39mset_postfix(loss\u001b[38;5;241m=\u001b[39mloss\u001b[38;5;241m.\u001b[39mitem())\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\annotated-transformer\\lib\\site-packages\\torch\\optim\\optimizer.py:88\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[1;32m---> 88\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\annotated-transformer\\lib\\site-packages\\torch\\autograd\\grad_mode.py:27\u001b[0m, in \u001b[0;36m_DecoratorContextManager.__call__.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclone():\n\u001b[1;32m---> 27\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\annotated-transformer\\lib\\site-packages\\torch\\optim\\adam.py:141\u001b[0m, in \u001b[0;36mAdam.step\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    138\u001b[0m             \u001b[38;5;66;03m# record the step after step update\u001b[39;00m\n\u001b[0;32m    139\u001b[0m             state_steps\u001b[38;5;241m.\u001b[39mappend(state[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m--> 141\u001b[0m     \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[43m           \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[43m           \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    144\u001b[0m \u001b[43m           \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    145\u001b[0m \u001b[43m           \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    146\u001b[0m \u001b[43m           \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    147\u001b[0m \u001b[43m           \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    148\u001b[0m \u001b[43m           \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    149\u001b[0m \u001b[43m           \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    150\u001b[0m \u001b[43m           \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[43m           \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    152\u001b[0m \u001b[43m           \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    153\u001b[0m \u001b[43m           \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[1;32mD:\\Anaconda\\envs\\annotated-transformer\\lib\\site-packages\\torch\\optim\\_functional.py:97\u001b[0m, in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m     94\u001b[0m     grad \u001b[38;5;241m=\u001b[39m grad\u001b[38;5;241m.\u001b[39madd(param, alpha\u001b[38;5;241m=\u001b[39mweight_decay)\n\u001b[0;32m     96\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m---> 97\u001b[0m \u001b[43mexp_avg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmul_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrad\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     98\u001b[0m exp_avg_sq\u001b[38;5;241m.\u001b[39mmul_(beta2)\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad\u001b[38;5;241m.\u001b[39mconj(), value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m amsgrad:\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;66;03m# Maintains the maximum of all 2nd moment running avg. till now\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training the model\n",
    "# Set the device to CUDA if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# Model setting\n",
    "model = BertModel.from_pretrained(model_name).to(device)\n",
    "model.train() \n",
    "sense_embeddings = sense_embeddings.to(device)  # Move sense embeddings to GPU\n",
    "loss_fn = nn.CosineEmbeddingLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
    "\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 3\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    progress_bar = tqdm(dataloader, desc=f'Epoch {epoch+1}/{num_epochs}', leave=True, position=0)\n",
    "    for batch in progress_bar:\n",
    "        # Send batch data to the device (GPU)\n",
    "        batch_input_ids, batch_attention_masks, batch_word_indices, batch_sense_indices = [b.to(device) for b in batch]\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(input_ids=batch_input_ids, attention_mask=batch_attention_masks)\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "        \n",
    "        # Retrieve embeddings for specific word indices\n",
    "        word_embeddings = torch.stack([hidden_states[i, idx, :] for i, idx in enumerate(batch_word_indices)])\n",
    "        \n",
    "        # Retrieve the corresponding sense embeddings\n",
    "        target_embeddings = sense_embeddings[batch_sense_indices]\n",
    "\n",
    "        # Labels tensor indicating that embeddings should be similar\n",
    "        labels = torch.ones(word_embeddings.size(0), device=device)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = loss_fn(word_embeddings, target_embeddings, labels)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, Loss: {total_loss / len(dataloader)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66602e3d-1a2c-4982-8a2a-44343d0db00a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0eca322-1906-4926-9749-c76e00415b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting for the model\n",
    "# Could be problem here, as we always find the first one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5343c8e3-a516-4650-98cf-af3ee71c3611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare time\n",
    "import time\n",
    "# Function to time the execution\n",
    "def time_function(func, file_path):\n",
    "    start_time = time.time()\n",
    "    func(file_path)\n",
    "    end_time = time.time()\n",
    "    return end_time - start_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8536942f-16b0-4671-8939-3059ab43f57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Original function\n",
    "def load_ball_embeddings_original(bFile):\n",
    "    print(\"loading balls....\")\n",
    "    bdic = {}\n",
    "    with open(bFile, 'r') as w2v:\n",
    "        for line in w2v.readlines():\n",
    "            wlst = line.strip().split()\n",
    "            bdic[wlst[0]] = [decimal.Decimal(ele) for ele in wlst[1:]]\n",
    "    print(len(bdic), ' balls are loaded\\n')\n",
    "    return bdic\n",
    "\n",
    "# Optimized function\n",
    "def load_ball_embeddings_optimized(bFile):\n",
    "    print(\"loading balls....\")\n",
    "    bdic = {}\n",
    "    with open(bFile, 'r') as w2v:\n",
    "        for line in w2v:\n",
    "            wlst = line.split()\n",
    "            bdic[wlst[0]] = list(map(float, wlst[1:]))\n",
    "    print(len(bdic), ' balls are loaded\\n')\n",
    "    return bdic\n",
    "\n",
    "# Path to the embeddings file\n",
    "file_path = nball_small_path  # Update this to your actual file path\n",
    "\n",
    "# Time both functions\n",
    "original_time = time_function(load_ball_embeddings_original, file_path)\n",
    "optimized_time = time_function(load_ball_embeddings_optimized, file_path)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Original function time: {original_time} seconds\")\n",
    "print(f\"Optimized function time: {optimized_time} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35861c08-49e4-4608-b2a2-2842b2e0ef6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import swifter\n",
    "# Function to apply formatting without caching\n",
    "def process_without_cache(df):\n",
    "    df['formatted_sense_id'] = df['sense_id'].apply(lambda x: wn.lemma_from_key(x).synset().name())\n",
    "\n",
    "# Function to apply formatting with caching\n",
    "sense_id_cache = {}\n",
    "def process_with_cache(df):\n",
    "    def format_sense_id(sense_id):\n",
    "        if sense_id not in sense_id_cache:\n",
    "            sense_id_cache[sense_id] = wn.lemma_from_key(sense_id).synset().name()\n",
    "        return sense_id_cache[sense_id]\n",
    "    df['formatted_sense_id'] = df['sense_id'].apply(format_sense_id)\n",
    "\n",
    "# Function to apply formatting with swifter\n",
    "def process_with_swifter(df):\n",
    "    df['formatted_sense_id'] = df['sense_id'].swifter.apply(lambda x: wn.lemma_from_key(x).synset().name())\n",
    "\n",
    "# Timing each method\n",
    "time_no_cache = time_function(process_without_cache, semcor_training_merged.copy())\n",
    "time_cache = time_function(process_with_cache, semcor_training_merged.copy())\n",
    "time_swifter = time_function(process_with_swifter, semcor_training_merged.copy())\n",
    "\n",
    "print(\"Time without caching:\", time_no_cache)\n",
    "print(\"Time with caching:\", time_cache)\n",
    "print(\"Time with swifter:\", time_swifter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0322f18-d6b7-4ebb-ace0-a4367d8197a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the tokenizer once, to be used across functions\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Define the original method as a function\n",
    "def original_method(df):\n",
    "    def tokenize_and_find_index(row):\n",
    "        sentence = str(row['sentence_text'])\n",
    "        word = str(row['word'])\n",
    "        tokens = tokenizer(sentence, padding='max_length', max_length=512, truncation=True, return_tensors=\"pt\")\n",
    "        input_ids = tokens['input_ids'][0]\n",
    "        word_tokens = tokenizer.tokenize(word)\n",
    "        for i in range(len(input_ids) - len(word_tokens) + 1):\n",
    "            if input_ids[i:i+len(word_tokens)].tolist() == tokenizer.convert_tokens_to_ids(word_tokens):\n",
    "                return tokens['input_ids'], tokens['attention_mask'], i\n",
    "        return tokens['input_ids'], tokens['attention_mask'], -1\n",
    "    df[['input_ids', 'attention_mask', 'word_index']] = df.apply(tokenize_and_find_index, axis=1, result_type='expand')\n",
    "\n",
    "# Define the optimized method as a function\n",
    "def optimized_method(df):\n",
    "    tokenized_data = tokenizer(list(df['sentence_text']), padding=True, truncation=True, return_tensors=\"pt\", max_length=512)\n",
    "    input_ids = tokenized_data['input_ids']\n",
    "    attention_masks = tokenized_data['attention_mask']\n",
    "    word_indices = [\n",
    "        find_word_index(sentence_ids, word)\n",
    "        for sentence_ids, word in zip(input_ids, df['word'])\n",
    "    ]\n",
    "    df['input_ids'] = input_ids\n",
    "    df['attention_mask'] = attention_masks\n",
    "    df['word_index'] = word_indices\n",
    "\n",
    "# Function to find word index used in optimized method\n",
    "def find_word_index(sentence_ids, word):\n",
    "    word_tokens = tokenizer.tokenize(word)\n",
    "    word_ids = tokenizer.convert_tokens_to_ids(word_tokens)\n",
    "    for i in range(len(sentence_ids) - len(word_tokens) + 1):\n",
    "        if sentence_ids[i:i+len(word_tokens)].tolist() == word_ids:\n",
    "            return i\n",
    "    return -1\n",
    "\n",
    "# Function to time the execution\n",
    "def time_function(func, df):\n",
    "    start_time = time.time()\n",
    "    func(df)\n",
    "    end_time = time.time()\n",
    "    return end_time - start_time\n",
    "\n",
    "# Make copies of the DataFrame and measure execution time\n",
    "df_copy_for_original = semcor_training_merged.copy()\n",
    "df_copy_for_optimized = semcor_training_merged.copy()\n",
    "time_original = time_function(original_method, df_copy_for_original)\n",
    "time_optimized = time_function(optimized_method, df_copy_for_optimized)\n",
    "\n",
    "print(f\"Original Method Time: {time_original} seconds\")\n",
    "print(f\"Optimized Method Time: {time_optimized} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee62cd1-d9c3-4f5f-afb9-1bf7e2e64543",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (annotated-transformer)",
   "language": "python",
   "name": "annotated-transformer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
