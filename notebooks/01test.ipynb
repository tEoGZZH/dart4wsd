{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "78fd8809-7259-488e-978f-8a71a8731e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "575a76cb-9f95-412d-8e2c-22bb4659efbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For the 0-th sentence, the index of Apple is: 3\n",
      "For the 1-th sentence, the index of Apple is: 4\n",
      "For the 2-th sentence, the index of Apple is: 1\n",
      "For the 3-th sentence, the index of Apple is: 2\n",
      "Contextual embeddings for 'apple' extracted from each sentence.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "# Initialize the tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Custom dataset with the word \"apple\" in different contexts\n",
    "sentences = [\n",
    "    \"I love apple pies.\",\n",
    "    \"He bought an apple from the store.\",\n",
    "    \"Apple announced a new product yesterday.\",\n",
    "    \"The apple tree in my garden is blooming.\"\n",
    "]\n",
    "\n",
    "# Tokenize sentences and prepare input\n",
    "inputs = tokenizer(sentences, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "\n",
    "# Move to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "# Get the outputs (last hidden states)\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "# Extract embeddings for the word \"apple\" from each sentence\n",
    "apple_embeddings = []\n",
    "for i, sentence in enumerate(sentences):\n",
    "    # Find the index of the word \"apple\" in each sentence\n",
    "    apple_index = inputs['input_ids'][i].tolist().index(tokenizer.encode('apple', add_special_tokens=False)[0])\n",
    "    # Extract the embedding for this index\n",
    "    print(f\"For the {i}-th sentence, the index of Apple is: {apple_index}\")\n",
    "    apple_embedding = outputs.last_hidden_state[i, apple_index]\n",
    "    apple_embeddings.append(apple_embedding)\n",
    "\n",
    "# Now, apple_embeddings contains the contextual embeddings for the word \"apple\" in each sentence\n",
    "print(\"Contextual embeddings for 'apple' extracted from each sentence.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f738f8-f190-4e1b-8308-a78de679f2e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1f4bcca-ee6c-4f0b-82cc-d28cdba17ad5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[101, 1045, 2293, 6207, 11345, 2015, 1012, 102, 0, 0, 0, 0]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['input_ids'][0].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28a28948-c736-411f-a8ba-05240ff925e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6207]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.encode('apple', add_special_tokens=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a440c8-bca2-4c96-9916-b947a00d6138",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load a sample dataset and take a small subset for quick training\n",
    "dataset = load_dataset('glue', 'mrpc')\n",
    "train_dataset = dataset['train'].select(range(100))  # Use only the first 100 samples\n",
    "\n",
    "# Initialize the tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenization\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['sentence1'], examples['sentence2'], padding=\"max_length\", truncation=True)\n",
    "\n",
    "train_dataset = train_dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Print some details about the tokenized dataset\n",
    "print(\"Sample tokenized input:\", train_dataset[0])\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Move model to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Training loop\n",
    "model.train()\n",
    "for epoch in range(3):  # Looping for 3 epochs\n",
    "    for batch in train_loader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items()}\n",
    "        outputs = model(**batch)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        print(f\"Epoch {epoch + 1}, Training loss: {loss.item()}\")\n",
    "\n",
    "# Save the fine-tuned model\n",
    "model.save_pretrained(\"./fine_tuned_bert\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "723ef78d-f998-4511-8c96-a317e3e187f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data\n",
    "texts = [\"The bank will not approve my loan.\", \"We sat on the river bank.\"]\n",
    "target_words = [\"bank\", \"bank\"]\n",
    "contexts = [(text, word) for text, word in zip(texts, target_words)]\n",
    "\n",
    "# Custom dataset class\n",
    "class ContextDataset(Dataset):\n",
    "    def __init__(self, contexts):\n",
    "        self.contexts = contexts\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.contexts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text, word = self.contexts[idx]\n",
    "        inputs = self.tokenizer(text, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=128)\n",
    "        word_tokens = self.tokenizer.tokenize(word)\n",
    "        word_index = inputs[\"input_ids\"].squeeze().tolist().index(self.tokenizer.convert_tokens_to_ids(word_tokens)[0])\n",
    "        return {\n",
    "            'input_ids': inputs['input_ids'].squeeze(),\n",
    "            'attention_mask': inputs['attention_mask'].squeeze(),\n",
    "            'word_index': word_index\n",
    "        }\n",
    "\n",
    "# Custom collate function\n",
    "def custom_collate(batch):\n",
    "    input_ids = torch.stack([item['input_ids'] for item in batch])\n",
    "    attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
    "    word_indices = torch.tensor([item['word_index'] for item in batch])\n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'word_index': word_indices\n",
    "    }\n",
    "\n",
    "dataset = ContextDataset(contexts)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True, collate_fn=custom_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f35b12c-6d84-4e7d-81fc-7a0143561f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example sense embeddings for the word \"bank\"\n",
    "sense_embeddings = {\n",
    "    \"financial_institution\": np.random.rand(768).astype(np.float32),\n",
    "    \"river_side\": np.random.rand(768).astype(np.float32)\n",
    "}\n",
    "sense_labels = list(sense_embeddings.keys())\n",
    "sense_tensor = torch.tensor([sense_embeddings[label] for label in sense_labels])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d53d4965-75e9-4bfe-9496-1ac4920dbaa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\annotated-transformer\\lib\\site-packages\\torch\\nn\\modules\\loss.py:529: UserWarning: Using a target size (torch.Size([2])) that is different to the input size (torch.Size([2, 2])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 486.03436279296875\n",
      "Epoch 2, Loss: 419.296142578125\n",
      "Epoch 3, Loss: 366.17523193359375\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Load BERT model\n",
    "model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "loss_fn = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-5)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(3):  # Train for 3 epochs\n",
    "    model.train()\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        input_ids = batch['input_ids']\n",
    "        attention_mask = batch['attention_mask']\n",
    "        word_index = batch['word_index']\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(input_ids, attention_mask=attention_mask)\n",
    "        hidden_states = outputs.last_hidden_state\n",
    "\n",
    "        # Get the word embeddings\n",
    "        word_embeddings = hidden_states[range(hidden_states.size(0)), word_index]\n",
    "\n",
    "        # Calculate the distance to each sense embedding\n",
    "        distances = torch.cdist(word_embeddings.unsqueeze(0), sense_tensor.unsqueeze(0), p=2).squeeze()\n",
    "\n",
    "        # Calculate the loss\n",
    "        target = torch.zeros(word_embeddings.size(0), dtype=torch.long)\n",
    "        loss = loss_fn(distances, target.float())\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fec4afff-9fb1-4dc1-9836-e54762b66ce3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The sense of 'bank' in the context is: river_side\n"
     ]
    }
   ],
   "source": [
    "def match_sense(text, word):\n",
    "    model.eval()\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding='max_length', truncation=True, max_length=128)\n",
    "    outputs = model(**inputs)\n",
    "    hidden_states = outputs.last_hidden_state\n",
    "\n",
    "    # Get the word index\n",
    "    word_tokens = tokenizer.tokenize(word)\n",
    "    word_index = inputs[\"input_ids\"].squeeze().tolist().index(tokenizer.convert_tokens_to_ids(word_tokens)[0])\n",
    "    \n",
    "    # Get the word embedding\n",
    "    word_embedding = hidden_states[0, word_index, :].unsqueeze(0).float()\n",
    "    \n",
    "    # Calculate distances to sense embeddings\n",
    "    distances = torch.cdist(word_embedding, sense_tensor.unsqueeze(0), p=2).squeeze()\n",
    "    closest_sense_index = torch.argmin(distances).item()\n",
    "\n",
    "    return sense_labels[closest_sense_index]\n",
    "\n",
    "# Example usage\n",
    "text = \"The bank will not approve my loan.\"\n",
    "word = \"bank\"\n",
    "sense = match_sense(text, word)\n",
    "print(f\"The sense of '{word}' in the context is: {sense}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8506dc0f-8359-418e-93cf-7d3dee728c8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (annotated-transformer)",
   "language": "python",
   "name": "annotated-transformer"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
